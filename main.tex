\documentclass{report}
\input{preamble}
\input{macros}
\input{letterfonts}
\title{Appunti Lab}
\author{Francesco Sermi}
\date{}
\makeindex
\begin{document}
	\maketitle
	\tableofcontents
	\newpage
	\chapter{Introduzione}
	Il concetto di \emph{incertezza} e di \emph{misura} gioca un ruolo centrale nelle scienze sperimentali siccome noi \textbf{non} siamo mai in grado di misurare una grandezza fisica con un'accuratezza infinita, dunque il nostro risultato manca di una parte sostanziosa del proprio contenuto se non vi è assegnata una stima dell'incertezza compiuta nella misura effettuata. \\
	Il primo sforzo nella storia delle scienze verso un sistema standard di unità di misure è stato costituito, nella Francia del XVIII secolo, del sistema metrico decimale. Una domanda molto interessante che il lettore si potrebbe chiedere sarebbe la seguente \emph{"Come possiamo definire un'unità di misura?"} La risposta più semplice, ma non banale, è tramite dei campioni inalterabili e riproducibili: si riporta proprio la realizzazione del metro campione e del kilogrammo campione realizzati, sebbene subiscano una contaminazione, ogni anno, pari a $1 \si{\micro\gram}$. \\
	Oggi è stato definito il Sistema internazione delle misure che si basa sul fissare 7 unità di base da cui è possibile esprimere tutte le altre come combinazione delle altre (si rimanda la lettura delle dispense di Baldini per usufruire dei suoi grafici e/o tabelle). \\
	\section{L'errore massimo e le sue limitazioni}
	Si osserva che nella sua formulazione più elementare il concetto di \emph{errore massimo} è legato alla domanda \emph{"Qual è il più piccolo intervallo che contiene con certezza il valore numerico della quantità che sto misurando?"}. In altre parole, il risultato della misura di una generica grandezza fisica $x$ risulta essere pari a:
	\begin{equation}
		x = \hat{x} \pm \Delta x
		\label{err_max}
	\end{equation}
	intendo come $\Delta x$ l'errore massimo\index{errore massimo}\ignorespaces, ovvero stiamo dicendo che l'intervallo $[\hat{x} - \Delta x; \hat{x} + \Delta x]$ è il più piccolo intervallo possibile che ci dà la certezza di includere il valore incognito $x$. Definiamo adesso una serie di termini comodi per esprimere al meglio i concetti successivi:
	\begin{itemize}
		\item $x$ è il valore, incognito (siccome non lo conosciamo mai appieno), della grandezza che vogliamo misurare e che chiameremo \emph{misurando};
		\item $\hat{x}$ è la migliore stima di $x$ che possiamo fornire a partire dei dati a nostra disposizione (e che chiameremo \emph{valore centrale} o \emph{migliore stima} della nostra misura);
		\item $\Delta x$ è l'incertezza di misura e, in questo \textbf{caso}, coincide con l'\textbf{errore massimo}
	\end{itemize}
	Tuttavia, nel caso di misure ripetute, giungiamo ad un assurdo: infatti, se una misura fluttua, da un punto di vista operativo, non possiamo escludere che una nuova misura della grandezza non fornisca un valore al di fuori dell'intervallo iniziale di incertezza e, ove questo accade, siamo costretti ad allargare tale intervallo, giungendo, dunque, ad un evidente paradosso, siccome acquisire nuove informazioni può solo peggiore (o lasciare invariato, se otteniamo delle misure che fanno ancora parte di $[x-\Delta x; x + \Delta x]$) il nostro stato di conoscenza, almeno determinando l'incertezza come errore massimo. \\
	Questo è il motivo per cui non utilizziamo quasi mai il concetto di errore massimo, ma utilizzeremo il concetto di errore statistico.
	\begin{equation}
		x = \hat{x} \pm \sigma_x
		\label{err_stat}
	\end{equation}
	La \ref{err_stat} ha un significato diverso da \ref{err_max} prima siccome questa definisce un intervallo che non ci dà la \emph{certezza} ma solo la probabilità, ben definita, di contenere il valore del misurando. \\
	Adesso definiamo un altro modo molto utili per quantificare quanto è grosso, rispetto alla misura, l'errore che noi commettiamo:
	\dfn{Errore percentuale}{L'errore relativo\index{errore relativo} è il rapporto tra l'incertezza della misura e il suo valore centrale:
	\begin{equation}
		e_{\%} = \frac{\sigma_x}{|\hat{x}|}
\end{equation}}
	\section{Precisione vs accuratezza}
	E' molto sottile la distinzione fra la precisione di uno strumento e la sua accuratezza: con il primo termine si indica l'accordo tra il valore misurato dallo strumento e quello effettivo del misurando, mentre con il secondo si intende il grado di consistenza fra i risultati di misure successive della stessa quantità nelle medesime condizioni
	\chapter{Probabilità}
	Grazie al matematico Kolmogorov abbiamo la prima costruzione rigorosa della teoria della probabilità, in una struttura che, sostanzialmente, sopravvive ancora oggi nei manuali moderni. \\
	La struttura di base su cui si fonda la struttura assiomatica della probabilità parte dal definire lo spazio campionario
	\dfn{Spazio campionario $\Omega$}{Lo spazio campionario\index{spazio campionario} $\Omega$ è l'insieme (numerabile) di tutte le possibili realizzazioni elementari di un dato fenomeno e lo spazio degli eventi\index{spazio degli eventi} $\mathcal{F}$ l'insieme di tutti i sottoinsiemi di $\Omega$ tale che:
	$$
		\# \mathcal{F} = 2^{\# \Omega}
	$$}
\noindent L'idea di Kolmogorov è quella di definire la probabilità\index{probabilità} direttamente sullo spazio degli eventi-cioè possiamo assegnare una probabilità non solo ad ogni elemento dello spazio campionario, ma anche ad uno qualsiasi dei suoi sottoinsiemi.
	Definiamo adesso il concetto di probabilità:
	\dfn{Probabilità}{Definiamo probabilità una misura P su $\mathcal{F}$ che associ univocamente ad ogni elemento E di $\mathcal{F}$ un numero reale $P(E)$ che soddisfa le seguenti tre proprietà (o \textbf{assiomi di Kolmogorov}\index{assiomi di Kolmogorov}\ignorespaces):
	\begin{enumerate}[label=\protect\circled{\arabic*}]
		\item $0 \leq P(E) \leq 1 \, \forall E \in \mathcal{F}$
		\item $P(\Omega) = 1$
		\item $P(E_1 \cup E_2) = P(E_1) + P(E_2) \, \text{se} \, E_1 \cap E_2 = \emptyset$
	\end{enumerate}		
	}
\noindent Possiamo utilizzare il terzo assioma di Kolgomorov all'unione numerabili di eventi \emph{disgiunti} (ovvero eventi per cui la loro intersezione è nulla)
	\cor{Unione numerabili di eventi disgiunti}{$P(\bigcup\limits_{i}^n E_i) = \sum\limits_{i}^n P(E_i)$ se $E_1 = E_2 = \cdots = E_n = 0$}
	\begin{myproof}
	Si procede per induzione su $n$. Per $n = 1$ è banale, siccome:
	$$
		P \left( \bigcup_i^1 E_i \right) = P(E_1) = \sum_i^1 P(E_i) = P(E_i)
	$$
	Adesso mostriamo $n \implies n+1$:
	$$
		\sum_i^{n+1} P(E_i) = P(E_1) + \cdots + P(E_{n+1}) = \text{(ip. induttiva)} \, P \left(\bigcup_i^n E_i \right) + P(E_{n+1}) = P \left( \sum_{i}^{n+1} E_i \right)
	$$
	Siccome possiamo vedere nell'ultimo passaggio la somma fra due eventi che sono fra loro disgiunti, ovvero fra l'evento $E_1 \cup \E_2 \cdots \cup E_n$ e l'evento $E_{n+1}$ e per ipotesi sappiamo che $E_i \cap E_j = \emptyset \, \forall i,j$. \\
	La dimostrazione è dunque conclusa.
	\end{myproof}
	\cor{Probabilità complementare}{Dato un evento E e detto $\bar{E}$ il suo complementare in $\mathcal{F}$, si ha che:
		$$
			P(\bar{E}) = 1 - P(E)
		$$
		dove $P(\bar{E})$ è detta \index{probabilità complementare} di $E$
	}
	\begin{myproof}
		Sapendo che $\bar{E} \cap E = \emptyset$ ma $\bar{E} \cup E = \Omega$:
		$$ 1 = P(\Omega) = P(\bar{E} + E) = P(\bar{E}) + P(E) \implies P(\bar{E}) = 1 - P(E)$$
	\end{myproof}
	\cor{Probabilità dell'insieme nullo}{$P(\emptyset) = 0$}
	\begin{myproof}
		$$
			P(\Omega) = P(\Omega \cup \emptyset)
		$$
		ma siccome $\Omega \cap \emptyset = \emptyset$ allora
		$$
			P(\Omega \cup \emptyset) = P(\Omega) + P(\emptyset) = P(\Omega) \implies 1 + P(\emptyset) = 1 \implies P(\emptyset) = 0
		$$
		La dimostrazione è dunque conclusa.
	\end{myproof}
	\cor{Limitatezza della probabilità di un sottoinsieme}{Se $E_1 \subset E_2 \implies P(E_1) \leq P(E_2)$}
	\begin{myproof}
		Se $E_1 \subset E_2 \implies E_2 = E_1 + (E_2 \setminus E_1)$ ma siccome $E_1 \cap (E_2 \setminus E_1) = \emptyset$:
		$$P(E_2) = P(E_1) + P(E_2 \setminus E_1) \implies P(E_1) \leq P(E_2)$$
	\end{myproof}
	\thm{Addizione delle probabilità}{Dati due eventi $E_1$ ed $E_2$, si ha che:
	\begin{equation}
		P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)
	\end{equation}			
	}
	\begin{myproof}
		Possiamo scrivere gli insiemi $E_1$ ed $E_2$ come unione di eventi disgiunti, infatti:
		$$
			E_2 = E_2 \cap \Omega = E_2 \cap (E_1 \cup \bar{E_1}) = (E_2 \cap E_1) \cup (E_2 \cap \bar{E_1})
		$$
		ma siccome $(E_2 \cap E_1) \cap (E_2 \cap \bar{E_1}) = \emptyset$ (altrimenti si giungerebbe ad un assurdo, visto che se $E_2 \cap E_1$ avesse degli elementi in comune con $E_2 \cap \bar{E_1}$ implicherebbe, visto che l'intersezione fra insieme è un'operazione che gode di proprietà associativa e commutativa, che $E_1 \cap \bar{E_1} \cap E_2 \cap E_2 \neq \emptyset$ ma $E_1 \cap \bar{E_1} = \emptyset$, dunque assurdo). Tornando alla dimostrazione:
		\begin{equation}
			P(E_2) = P \left( (E_2 \cap E_1) \cup (E_2 \cap \bar{E_1}) \right) = P(E_2 \cap E_1) + P(E_2 \cap \bar{E_1})
		\end{equation}
		d'altra parte abbiamo che
		$$
			E_1 \cup E_2 = (E_1 \cup E_2) \cap \Omega = (E_1 \cup E_2) \cap (E_1 \cup \bar{E_1}) = (E_1 \cap E_1) \cup (E_1 \cap \bar{E_1}) \cup (E_2 \cap E_1) \cup (E_2 \cap \bar{E_1})
		$$ = $E_1 \cup (E_2 \cap E_1) \cup (E_2 \cap \bar{E_1})$
		ma si osserva che il termine $E_1 \cup (E_2 \cap E_1)$ è ridondante, siccome $E_1 \cup (E_2 \cap E_1) = E_1$, dunque
		\begin{align*}
			E_1 \cup E_2 = E_1 \cup (E_2 \cup \bar{E_1}) \implies P(E_1) = P(E_1) + P(E_2 \cup \bar{E_1})
		\end{align*}
		e combinando le due relazioni ottenute
		\begin{equation*}			
			\begin{cases}
				P(E_2) = P(E_2 \cap E_1) + P(E_2 \cap \bar{E_1}) \\
				P(E_1 \cup E_2) = P(E_1) \cup P(E_2 \cap \bar{E_1})
			\end{cases}
		\end{equation*}
		dunque
		$$
			P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)
		$$
	\end{myproof}
	\section{Definizione operativa della probabilità}
	Il lettore più attento si sarà però accorto di come la definizione che abbiamo dato di probabilità non ci dà alcun modo con cui calcolare la probabilità, ma piuttosto una serie di assiomi da cui possiamo ricavare una serie di proprietà utili di cui gode la probabilità.
	\subsection{Definizione combinatoriale}
	Nella sua definizione combinatoriale, definiamo la probabilità di un evento E con quanto segue
	\dfn{Definizione combinatoriale della probabilità}{La probabilità di un evento E coincide con il rapporto tra il numero di casi favorevoli $n$ e il numero di casi possibili $N$, \underline{a condizione che questi siano tutti equiprobabili}
	\begin{equation}
		P(E) = \frac{n}{N}
\end{equation}		
	}
	\nt{Si osserva che questa definizione di probabilità rispetta i tre assiomi di Kolmogorov: il primo assioma discende dalla ovvia condizione per cui $0 \leq n \leq N$ ed il secondo fatto deriva dal fatto che se $n=N \implies P(E) = 1$. Per quanto riguarda la terza condizione, si osserva che se $E_1$ e $E_2$ sono due eventi disgiunti con rispettivamente $n_1$ e $n_2$ casi favorevoli, allora:
		\begin{equation*}
		P(E_1 \cup E_2) = \frac{n_1 + n_2}{N} = \frac{n_1}{N} + \frac{n_2}{N} = P(E_1) + P(E_2)
\end{equation*}		
	dunque anche il terzo assioma è rispettato	
	}
\noindent Tuttavia questa definizione operativa di probabilità possiede un grande problema: nella definizione è compiuto infatti un ragionamento circolare, siccome richiediamo l'equiprobabilità dei casi nella definizione stessa di probabilità.
	\subsection{Definizione frequentista}
	Quando è possibile ripetere un esperimento in condizioni controllate, è possibile definire la probabilità di un evento E come il limite della frequenza relativa all'evento stesso quando il numero di ripetizioni $N$ dell'esperimento tende all'infinito. Possiamo quindi pensare di effettuare un esperimento un numero $N$ arbitrariamente grande di volte, contare le $n$ volte in cui è avvenuto l'evento E e definire la probabilità $P(E)$ dell'evento come il limite del rapporto $\frac{n}{N}$
	\dfn{Definizione frequentista della probabilità}{La probabilità di un evento E si definisce come
	\begin{equation}
		P(E) = \lim_{N \to +\infty} \frac{n}{N}
	\end{equation}
	dove il limite va inteso nei termini della convergenza statistica, ovvero
	\begin{equation}
		\forall \epsilon, \delta > 0 \exists \tilde{N} > 0:\forall N, N>\tilde{N} \implies P \left( \left|\frac{n}{N} - P(E) \right| \geq \delta \right) \leq \epsilon
	\end{equation}
	}
	\nt{Il senso di questo limite, che va inteso come limite in senso statistico piuttosto che nel senso usuale dell'analisi matematica, è il fatto che non è possibile garantire a priori l'esistenza di un numero $N$ di ripetizioni del nostro esperimento che mi permetta di affermare con certezza che la differenza tra la frequenza registrata $\frac{n}{N} - P(E)$ sia minore di una certa quantità $\epsilon$: infatti, se effettuiamo due diverse serie di $N$ ripetizioni dell'esperimento otterrò frequenze relative $\frac{n}{N}$ diverse. Quello che possiamo però dire è il fatto che se $N$ è abbastanza grande allora posso rendere piccola a piacere la probabilità che $\frac{n}{N}$ si discosti da $P(E)$ di un valore prefissato $\delta$.}
	\ex{Lancio di un dado}{Se lanciamo $N$ volte un dado equo a sei facce e registriamo (al crescere di N) il numero $n$ di volte in cui esce, ad esempio, il numero 3, per $N$ molto grande il rapporto $\frac{n}{M}$ tenderà a $P(3) = \frac{1}{3}$ (nel senso della convergenza statistica)}
	\subsection{Definizione soggettivista della probabilità}
	\dfn{Definizione soggettivista}{La probabilità di un evento E si identifica con la misura del grado di fiducia che un individuo attribuisce al verificarsi di E, sulla base dell'informazione a sua disposizione}
	\nt{Il termine "soggettivo" si riferisce al fatto che persone diverse, sulla base di differenti informazioni, assoceranno, in generale, una probabilità diversa allo stesso evento e, proprio per questo fatto, si dice che questa definizione è \emph{soggettiva}}
\noindent Alla fine, sebbene questa definizione lasci inizialmente sbigottiti siccome si perde quell'oggettività che, in un certo senso, assicuravano le altre due definizioni, questa definizione rispetto di fatto come noi operiamo nella vita di tutti i giorni.
	All'interno della scuola soggettivista vi sono diversi approcci distinti per derivare le regole fondamentali della probabilità in un modo logicamente consistente (sebbene, con questo \emph{approccio}, gli assiomi non sono tali, ma regole che si ricavano da un principio più formale): il più popolare di questi approcci è il principio della \textbf{scommessa coerente}, il quale afferma che \emph{una volta assegnata la probabilità ad un evento dovremo essere disposti ad accettare scommesse sul verificarsi dell'evento stesso con un rapporto tra puntata e vincita determinato dalla probabilità stessa}\footnote{il senso è che se diciamo che due eventi sono equiprobabili allora dobbiamo essere pronti ad accettare scommesse 1:1, ovvero che una \emph{scommessa} è coerente se e solo se non determina \textbf{a priori} una perdita per il banco o per lo scommettitore, dunque è equivalente anche se i ruoli fossero scambiati} \\
	\section{Elementi di calcolo combinatorio}
	La combinatoria è quella branca della matematica che si occupa del \emph{contare}, dunque è strettamente connessa alla probabilità. \\
	Introduciamo il fattoriale\index{fattoriale}\ignorespaces di un numero nella seguente maniera
	\dfn{Fattoriale}{Dato un numero $n \in \mathbb{N}$, indichiamo con $n!$
		\begin{equation}
			n! = \prod_{k=1}^{n} k = n(n-1) \cdots 1	
		\end{equation}			
	}
\noindent Se ci pensiamo bene, la funzione fattoriale non è altro il numero di \emph{permutazioni}, ovvero il numero di modi in cui si possono disporre $n$ elementi se non possiamo ripeterli e \textbf{conta} l'ordine con cui questi elementi vengono disposti: supponiamo infatti di avere $20$ oggetti e di volerli disporre all'interno di un cassetto, noi abbiamo ben $20!$ modi possibili per metterli, siccome per il primo "posto" del cassetto possiamo metterci uno dei $20$, nel secondo "posto" $19$ oggetti e così via; ottenendo ben $20!$ fattoriale di possibilità. \\
	Le permutazioni possono essere viste come un caso particolare delle disposizioni, ovvero i modi con cui è possibile disporre $n$ oggetti in $k$ posti: per esempio, riprendendo l'esempio di prima, se noi volessimo prendere da $20$ oggetti $5$, presi a caso da questi $20$, all'interno di un cassetto in maniera tale che conti l'ordine con cui li mettiamo, si osserva che nel primo \emph{posto} del cassetto abbiamo $20$ oggetti disponibili, nel secondo $19$, nel terzo $18$, nel quarto $17$ e nell'ultimo $16$; dunque affermare che i modi totali sono $\frac{20!}{15!} = 20 \cdot 19 \cdot 18 \cdot 17 \cdot 16$. Definiamo quindi una disposizione come
	\dfn{Disposizione di $n$ elementi e di ordine $k$}{Definiamo una disposizione\index{disposizione} di $n$ elementi e di ordine $k$ come il numero di modi con cui è possibile disporre $n$ elementi in $k$ slot, pari a 		\begin{equation}
		D_{n,k} = \frac{n!}{(n-k)!}
	\end{equation}
}
\noindent Tornando alla funzione fattoriale, è comoda l'approssimazione di Stirling\index{approssimazione di Stirling}\ignorespaces (di cui non daremo una dimostrazione) per cui:
\begin{equation}
	n! = \sqrt{2\pi n} \left( \frac{n}{e} \right)^n
\end{equation}
Il numero di modi con cui è possibile scegliere  $k$ elementi non ordinati è dato dal coefficiente binomiale\index{coefficiente binomiale} $n$ su $k$
\dfn{Coefficiente binomiale $n$ su $k$}{Il coefficiente binomiale $n$ su $k$ è definito come
	\begin{equation}
		\binom{n}{k} = \frac{n!}{k!(n-k)!}
	\end{equation}
}
\cor{}{$$ \binom{n}{k} = \binom{n}{n-k} $$}
\begin{myproof}
	Banalmente, si osserva che
	$$
		\binom{n}{k} = \frac{n!}{k!(n-k)!} = \frac{n!}{(n-k)!(n-(n-k))!} = \binom{n}{n-k}
	$$
\end{myproof}
\noindent Il coefficiente binomiale è strettamente connesso al triangolo di Pascal (o di Newton) e può essere anche caratterizzato attraverso la formula della potenza del binomio (o teorema binomiale). Per farlo però ci servirà il seguente lemma:
\lemma{Proprietà fondamentale del coefficiente binomiale}{$$
	\binom{n+1}{k+1} = \binom{n}{k} + \binom{n}{k+1}
	$$
}
\begin{myproof}
	\begin{align*}
		&\binom{n}{k+1} + \binom{n}{k} = \frac{n!}{(k+1)!(n-k-1)!} + \frac{n!}{k!(n-k)!} = \frac{n!}{(k+1)k!(n-k-1)!} + \frac{n!}{k!(n-k)(n-k-1)!} \\ &= \frac{n!}{k!(n-k-1)!} \left( \frac{1}{k+1} + \frac{1}{n-k} \right) = \frac{n!}{k!(n-k-1)!} \left( \frac{n -k + k + 1}{(k+1)(n-k)} \right) = \frac{n!}{k!(n-k-1)!} \cdot \frac{(n+1)}{(k+1)(n-k)} \\ &= \frac{(n+1)!}{(k+1)!(n-k)!} = \binom{n+1}{k+1}
	\end{align*}
\end{myproof}
\thm{Teorema binomiale\index{teorema binomiale}\ignorespaces}{Lo sviluppo della potenza $n$-esima del binomio $x_1 + x_2$ è pari a
\begin{equation}
	(x_1 + x_2)^n = \sum_{k=0}^n \binom{n}{k} x_1^{n-k}x_2^k
\end{equation}
}
\begin{myproof}
si può procedere per induzione. Innanzitutto si osserva che, per $n=0$, si ha che
$$
	(x_1 + x_2)^0 = 1 = \sum_{k=0}^{0} \binom{n}{k} x_1^{n-k}x_2^k = \binom{0}{0} x_1^{0} x_2^0 = 1
$$
dunque per $n=0$ l'ipotesi è verificata. Mostriamo che $n \implies n+1$:
$$
	(x_1 + x_2)^{n+1} = (x_1 + x_2)^n (x_1 + x_2) = \sum_{k=0}^{n} \left[ x_1^{n-k}x_2^k \right] \cdot (x_1+x_2) = \sum_{k = 0}^{n} x_1^{n+1-k}x_2^k + \sum_{k = 0}^{n} x_1^{n-k}x_2^{k+1}
$$
Da qua soffermiamoci sul primo termine: si osserva che
$$
	\sum_{k=0}^{n} x_1^{n+1-k} x_2^k = \binom{n}{0} x_1^{n+1} + \sum_{k=1}^{n} \binom{n}{k} x_1^{n+1-k}x_2^k = \binom{n}{0} x_1^{n+1} + \sum_{k=0}^{n-1} \binom{n}{k+1} x_1^{n+1-k-1} x_2^{k+1} = x_1^{n+1} + \sum_{k = 0}^{n-1} \binom{n}{k+1} x_1^{n}x_2^{k+1}
$$
in cui si è semplicemente effettuato un cambio di variabile di variabile $k' = k + 1$ (ma siccome gli indici sono muti abbiamo semplicemente indicato $k'$ sempre con $k$). Adesso andiamo al secondo termine:
$$
	\sum_{k=0}^{n} \binom{n}{k} x_1^{n-k}x_2^{k+1} = \sum_{k=0}^{n-1} \binom{n}{k} x_1^{n-k}x_2^{k+1} + \binom{n}{n} x_2^{n+1} = \sum_{k=0}^{n-1} \binom{n}{k} x_1^{n-k}x_2^{k+1} + x_2^{n+1}
$$
Tornando dunque alla relazione iniziale, si deve avere che
$$
	(x_1 + x_2)^{n+1} = x_1^{n+1} + \sum_{k = 0}^{n-1} \binom{n}{k+1} x_1^{n}x_2^{k+1} + \sum_{k=0}^{n-1} \binom{n}{k} x_1^{n-k}x_2^{k+1} + x_2^{n+1} = x_1^{n+1} + \sum_{k=0}^{n-1} \left[ \binom{n}{k} + \binom{n}{k+1} \right] x_1^{n-k}x_2^{k+1} + x_2^{n+1}
$$
dunque, siccome per proprietà dei coefficienti binomiali si ha che $\binom{n+1}{k+1} = \binom{n}{k} + \binom{n}{k+1}$, allora
$$
	(x_1 + x_2)^{n+1} = x_1^{n+1} + \sum_{k=0}^{n-1} \binom{n+1}{k+1} x_1^{n-k}x_2^{k+1} + x_2^{n+1} = x_1^{n+1} + \sum_{k=1}^{n} \binom{n+1}{k} x_1^{n+1-k}x_2^{k} + x_2^{n+1} = \sum_{k=0}^{n+1} \binom{n+1}{k} x_1^{n+1-k}x_2^{k}
$$
Dunque la tesi è stata dimostrata 
\end{myproof}
\noindent Il coefficiente binomiale è comodo per calcolarsi le potenze del $2$ siccome se $x_1 = x_2 = 1$ si ha che
$$
2^n = \sum_{k = 0}^{n} \binom{n}{k} 1^{n-k} 1^{k} = \sum_{k=0}^{n} \binom{n}{k}
$$
Il coefficiente binomiale può anche essere generalizzato supponendo di voler dividere un insieme di $n$ elementi in $m$ sottoinsiemi disgiunti, ciascuno con un numero $k_i$ di elementi, la cui unione costituisca l'insieme di partenza, dunque si deve avere che $\sum_{i=1}^{m} k_i = n$
Il numero di modi con cui si può effettuare tale suddivisione prende il nome di \textbf{\index{coefficiente multinomiale} $n$ su $k_1, \dots , k_m$}  e si basa sull'idea che abbiamo $n$ su $k_1$ modi per scegliere il primo sottoinsieme, $n-k_1$ su $k_2$ modi per scegliere il secondo sottoinsieme e così via, dunque:
$$
	\binom{n}{k_1, k_2, \dots, k_m} = \binom{n}{k_1} \binom{n-k_1}{k_2} + \dots + \binom{n-k_1 \dots - k_{m-1}}{k_m}
$$
Sempre come per il coefficiente binomiale, possiamo caratterizzare il coefficiente multinomiale come lo sviluppo per la potenza $n$ di un numero arbitrario di monomi nella seguente maniera:
\begin{equation*}
	(x_1 + x_2 + \dots + x_m)^n = \sum_{ \{ (k_1, k_2, \dots, k_m) : \sum\limits_{i=1}^{m} k_i = n \} } \binom{n}{k_1, \dots k_m} x_1^{k_1} x_2^{k_2} \dots x_m^{k_m}
\end{equation*}
\section{Probabilità condizionata}
Tutto ciò che è stato mostrato nella sezione precedente può essere comodo per calcolare la probabilità di determinati eventi\footnote{usufruendo della definizione combinatoriale della probabilità} e consiglio caldamente di guardare gli esempi proposti da Baldini nel suo libro, su cui non mi soffermerò. Diamo delle definizioni
\dfn{Probabilità condizionata}{Dati due eventi $E_1$ ed $E_2$, con $P(E_2) \neq 0$, definiamo la \emph{probabilità condizionata}\index{probabilità condizionata} $P(E_1 | E_2)$ di $E_1$ dato $E_2$ (cioé la probabilità che si verifichi $E_1$ nel caso in cui sappiamo già che si verifica$E_2$) come
	\begin{equation}
		P(E_1 | E_2) = \frac{P(E_1 \cap E_2)}{P(E_2)}
	\end{equation}
}
\nt{La probabilità condizionata può essere vista come una sorta di "misura" dell'intersezione $E_1 \cap E_2$ pesata a $E_2$. E' possibile inoltre dimostrare che la probabilità condizionata soddisfa tutti gli assiomi della probabilità siccome ogni ogni evento $E$ può essere visto come una probabilità condizionata alla probabilità dello spazio campionario $P(E | \Omega)$}
\noindent L'utilità della probabilità condizionata è il fatto che possiamo calcolare la probabilità che due eventi $E_1$ ed $E_2$ si verifichino contemporaneamente nella seguente maniera:
$$
	P(E_1 \cap E_2) = P(E_1)P(E_2 | E_1) = P(E_2)P(E_1 | E_2)
$$
A questo punto diamo la definizione di eventi indipendenti \index{eventi indipendenti}
\dfn{Eventi indipendenti}{Si dice che due eventi $E_1$ ed $E_2$ sono \textbf{indipendenti} se il fatto che sia verificato $E_2$ non influenza la probabilità che si verifichi $E_1$, ovvero se
\begin{equation}
	P(E_1 | E_2) = P(E_1) \, \text{e} \, P(E_2 | E_1) = P(E_2)
\end{equation}
e per la definizione di probabilità condizionata possiamo riscrivere che
\begin{equation}
	P(E_1 \cap E_2) = P(E_1)P(E_2)
\end{equation}
}
\nt{Non bisogna confondere il concetto di \emph{\index{eventi indipendenti}} con il concetto di \emph{eventi disgiunti} e, dunque, incompatibili: si rimanda all'esempio di Baldini sul libro, in cui si osserva proprio il classico esempio del mazzo da carte in cui viene inserito un jolly. Prima dell'aggiunta del jolly i due eventi $E_1$="pesco un re" ed $E_2$="pesco una carta di cuori" erano indipendenti pure avendo un'intersezione non nulla, dopo l'aggiunta del jolly non sono più indipendenti ma non sono nemmeno incompatibili}
\section{Teorema di Bayes}
La naturale conseguenza di tutto ciò che abbiamo detto sulla probabilità condizionata è il teorema di Bayes\index{teorema di Bayes}, che lega tra loro la probabilità condizionata $P(E_1 | E_2)$ con la probabilità $P(E_2 | E_1)$
\thm{Teorema di Bayes}{La probabilità condizionata di $E_1$ dato $E_2$ è pari a
	\begin{equation}
		P(E_1 | E_2) = \frac{P(E_2 | E_1)P(E_1)}{P(E_2)}
	\end{equation}
dove $P(E_2 | E_1)$ è la probabilità condizionata di $E_2$ dato $E_1$
}
\begin{myproof}
	Dalla definizione di probabilità condizionata $P(E_1 | E_2)$ di $E_1$ dato $E_2$ sappiamo che
	$$
		P(E_1 | E_2) = \frac{P(E_1 \cap E_2)}{P(E_2)}
	$$
	con $P(E_2) \neq 0$, ma noi sappiamo pure che $P(E_1 \cap E_2) = P(E_2 | E_1)P(E_1)$, dunque si ottiene la tesi:
	$$
		P(E_1 | E_2) = \frac{P(E_2 | E_1)P(E_1)}{P(E_2)}
	$$
	La dimostrazione è dunque conclusa
\end{myproof}
\noindent Sfruttando inoltre il fatto che $E_1 \cup \bar{E_1} = \Omega$ allora $(E_1 \cap E_2) \cup (\bar{E_1} \cap E_2) = E_2 \cap (E_1 \cup \bar{E_1}) = E_2$, dunque:
$$
	P(E_2) = P(E_1 \cap E_2) + P(\bar{E_1} \cap E_2)
$$
(si ricorda che $(E_1 \cup E_2) \cup (\bar{E_1} \cup E_2) = \emptyset$ siccome se, per assurdo, $\exists c \in (E_1 \cap E_2) \cap (\bar{E_1} \cap E_2) \neq \emptyset \implies (c \in E_1 \cap E_2  \wedge c \in \bar{E_1} \cap E_2 )$ ma ciò implica che $(c \in E_1 \wedge c \in \bar{E_1})$ che è un assurdo; dunque possiamo usare il terzo assioma di Kolmogorov per stimare la probabilità di $E_2$). Ciò implica che
\begin{equation}
	P(E_1 | E_2) = \frac{P(E_2 | E_1)P(E_1)}{P(E_1 \cap E_2) + P(\bar{E_1} \cap E_2)}
\end{equation}
Più in generale, possiamo dire che se si dispone di un partizionamento dell'insieme $ \{ A_i \} $ (tali che $A_i \cap A_j = \emptyset \, \, \forall i, j, i \neq j$ e $\bigcup\limits_{i} A_i = \Omega$) allora possiamo generalizzare, per induzione su $i$, che $P(E_2) = \sum\limits_{i} P(E_2 | A_i)P(A_i)$, dunque:
\cor{Caratterizzazione di un evento tramite le partizioni}{Sia $\Omega$ lo spazio campionario degli eventi e sia $\{ E_i \}$ con $i \in I=[1, \dots, m], m \leq \#\mathcal{F}$ un partizionamento\footnote{ricordiamo che un partizione, per essere tale, deve risultare che $\forall i, j, i \neq j \, E_i \cap E_j = \emptyset$ e $\bigcup E_i = \mathcal{F}$} dell'insieme $\mathcal{F} = P(\Omega)$, ovvero lo spazio degli eventi, allora, dato un evento $A$, risulta che
$$
	A = (A \cap E_1) \cup (A \cap E_2) \dots (A \cap E_m)
$$
}
\begin{myproof}
	Si ha che $\bigcup\limits_i E_i = \mathcal{F}$ e siccome l'evento $A = A \cap \mathcal{F} = A \cap \bigcup\limits_i E_i = A \cap (E_1 \cup E_2 \dots E_m) = (A \cap E_1) \cup (A \cap E_2) \dots (A \cap E_m) = \bigcup\limits_i (A \cap E_i)$. Dimostriamo che gli $(A \cap E_i)$ sono tutti insiemi disgiunti $\forall i$: infatti se, per assurdo, $\exists c \in \mathcal{F} : \exists \tilde{i}, \tilde{d} \in I | (A \cap E_{\tilde{i}}) \cap (A \cap E_{\tilde{d}}) = c \neq \emptyset \implies (c \in (A \cap E_{\tilde{i}}) \wedge c \in (A \cap E_{\tilde{d}}))$ ma, per la definizione dell'operazione $\cap$, implica che $c \in E_{\tilde{i}} \wedge c \in E_{\tilde{d}}$ il che è un assurdo siccome gli $\{ E_i \}$ sono un partizionamento dell'insieme e, dunque, disgiunti.
	La dimostrazione è dunque conclusa
\end{myproof}
\noindent Andiamo adesso a scrivere il teorema di Bayes alla luce del corollario qua sopra
\begin{equation}
	P(A_1 | E_2) = \frac{P(E_2 | A_1)P(A_1)}{\sum\limits_{i} P(E_2 | A_i)P(A_i)}
\end{equation}
Questo teorema, sebbene risulti a prima vista banale, in realtà è molto importante siccome lega la probabilità diretta al problema di probabilità inversa. Si rimanda sempre al libro di Baldini per degli esempi per comprendere meglio il teorema di Bayes, sebbene sia utile osservare una cosa: mentre lo statista, oppure il matematico, studia la probabilità in astratto, ovvero cerca di calcolare la probabilità di un determinato processo "contando"; il fisico, di professione, cerca di inferire sul calcolo della probabilità attraverso delle raccolte dati, dunque tramite queste relazioni possiamo eventualmente ricondurci alla probabilità inversa, tuttavia cambia l'approccio "metodologico".
\section{Variabili casuali e funzioni di distribuzione}
Iniziamo questa sezione introducendo il concetto di variabile casuale
\dfn{Variabile casuale}{Una \emph{variabile casuale}\index{variabile casuale} o \emph{variabile aleatoria}\index{variabile aleatoria} è una variabile che rappresenta la realizzazione numerica di un processo causale, per cui il suo valore è soggetto a fluttuazioni casuali e non è noto a priori. Si distinguono in
\begin{itemize}
	\item \underline{discrete}, cioè variabili che possono assumere un numero finito o numerabile di valori;
	\item \underline{continue}, cioè variabili che possono assumere tutti i valori compresi in un intervallo.
\end{itemize}
}
\ex{}{\begin{itemize}
	\item L'uscita del lancio di un dado a sei facce è una variabile casuale discreta che può assumere esattamente sei valori: ${1, 2, 3, 4, 5, 6}$
	\item Il tempo necessario per arrivare da casa al luogo di lavoro è un esempio di variabile casuale continua
\end{itemize}}
\noindent Occupiamoci inizialmente di una variabile discreta $x$ che può assumere $n$ valori distinti $x_1, \dots, x_n$ e indichiamo con $P(x_k)$ la probabilità che assuma il valore $x_k$ e definiamo la \index{funzione di distribuzione}\footnote{in letteratura, si indica con questa espressione anche la funzione cumulativa di una distribuzione. Nel caso continuo, un termine che useremo sarà funzione di densità di probabilità} nella seguente maniera
\dfn{Funzione di distribuzione di $x$}{La \emph{funzione di densità di probabilità} è la funzione che associa ad ogni valore di $x_k$ della variabile la sua probabilità $P(x_k)$}
\noindent In questo contesto il secondo assioma di Kolmogorov si scrive nella forma di una \emph{\index{condizione di normalizzazione}}, che tutte le distribuzioni di distribuzione devono rispettare:
$$
	\sum_k P(x_k) = 1
$$
Nel caso di una variabile continua la definizione di funzione di distribuzione data per una variabile discreta fallisce clamorosamente siccome la probabilità che la variabile aleatoria $x$ assuma un valore \emph{esattamente} definito è zero (infatti mostreremo che si tratta di un integrale su un dominio di misura nulla). Ha però senso chiedersi qual è la probabilità, dato un punto generico $x_0$, che la variabile assuma un valore appartenente all'intervallo $[x_0, x_0 + dx]$
$$
	P(x_0, dx) = P(x_0 \leq x < x_0 + dx)
$$
Se a questo punto noi dividiamo per la larghezza dell'intervallo e consideriamo il limite per $dx \to 0$ otteniamo una sorta di probabilità specifica o probabilità specifica per unità di intervallo che chiamiamo \emph{\index{densità di probabilità}}:
\begin{equation}
	p(x_0) = \lim_{dx \to 0} \frac{P(x_0, dx)}{dx}
\end{equation}
che si tratta di una sorta di rapporto incrementale, per cui possiamo dire che la funzione di densità di probabilità è, in un certo senso, la derivata dalla funzione probabilità, dunque possiamo dire che la probabilità che la variabile aleatoria continua assuma un valore compreso nell'intervallo $[x_0, x_0 + dx]$ risulta essere pari
$$
	P(x_0, dx) = p(x_0)dx
$$
e, dunque, la probabilità che assuma un valore compreso nell'intervallo chiuso e limitato $[x_1, x_2]$ è pari a
\begin{equation}
	P(x_1, x_2) = \int_{x_1}^{x_2} p(x)dx
\end{equation}
In questo caso la \index{condizione di normalizzazione}, nel caso di una variabile aleatoria continua, si scrive come
\begin{equation}
	\int_{-\infty}^{+\infty} p(x)dx = 1
\end{equation}
\nt{Si osserva che mentre la probabilità che la variabile casuale assuma un valore contenuto in $[x_0, x_0+dx]$ sia un numero adimensionale, nel caso della funzione di densità di probabilità questa dimensionalmente deve avere dimensione pari all'inverso di $x$}
\section{Valore di aspettazione, varianza e momenti di una distribuzione}
Nel momento in cui andiamo a fare un'indagine statistica o, nel caso nostro, effettuiamo un esperimento è comodo condensare le informazioni contenute nella funzione di distribuzione in una serie di parametri significativi come il valore che assume in media la nostra distribuzione oppure quanto la funzione di distribuzione si discosta in media da questo valore. \\
\noindent Andiamo, proprio per questo, a definire il valore di aspettazione, primo strumento utile per caratterizzare i parametri d'interesse di una distribuzione:
\dfn{Valore di aspettazione di $f(x)$}{Sia data una variabile aleatoria $x$ e una funzione $f(x)$, definiamo il valore di aspettazione\index{valore di aspettazione} come
\begin{equation}
	E[f(x)] = \begin{cases} \sum\limits_{k} f(x_k)P(x_k) \, \text{nel caso di variabili discrete} \\ \int_{-\infty}^{+\infty} f(x)p(x)dx \, \text{nel caso di variabili continue} \end{cases}
\end{equation}
}
\nt{Il valore di aspettazione, alla fine, non è altro che una sorta di media "pesata" (con la probabilità che la variabile casuale assuma il valore $x_k$) della funzione $f(x)$ calcolata in $x_k$}
\noindent Inoltre, il valore di aspettazione è un \emph{operatore lineare}, siccome
$$
	E[c_1 f(x) + c_2 g(x)] = \int_{-\infty}^{+\infty} (c_1 f(x) + c_2 g(x))dx = c_1 \int_{-\infty}^{+\infty} f(x)dx + c_2 \int_{-\infty}^{+\infty} g(x)dx = c_1 E[f(x)] + c_2 E[g(x)]
$$
Inoltre il valore di aspettazione di una costante è pari alla costante stessa, siccome
$$
	E[c] = \int_{-\infty}^{+\infty} cdx = c\int_{-\infty}^{+\infty} dx = c \, \text{per la condizione di normalizzazione}
$$
Definiamo a questo punto il valore medio di una variabile casuale $x$ (continua o discreta) come
\dfn{Valore medio di una variabile causale $x$}{Il valore medio\index{valore medio} di una variabile casuale $x$ si definisce come il valore di aspettazione di $x$
\begin{equation}
	\mu = E[x] = \begin{cases} \sum\limits_{k} x_kP(x_k) \\ \int_{-\infty}^{+\infty} xp(x)dx \end{cases}
\end{equation}
}
\noindent Naturalmente, se $c$ è una costante allora
$$
	E[cx] = cE[x]
$$
per linearità dell'integrale.
\nt{Nel caso particolare di una variabile casuale e discreta per cui si abbiano $n$ uscite equiprobabili $x_k$ equiprobabili (ovvero $P(x_1) = P(x_2) = \dots = P(x_n) = \frac{1}{n}$ si ha che
$$
	\mu = \sum_{k = 1}^{n} x_k P(x_k) = \frac{1}{n}\sum_{k=1}^{n} P(x_k)
$$
}
\noindent Tuttavia il valore medio non è l'unica stima possibile di tendenza centrale: possiamo definire anche la \emph{\index{mediana}}:
\dfn{Mediana}{Si definisce \textbf{mediana}\index{mediana} di una distribuzione quel valore $\mu_{\frac{1}{2}}$ della variabile casuale tale che
$$
	P(x \leq \mu_{\frac{1}{2}}) = P(x \geq \mu_{\frac{1}{2}})
$$
}
\noindent Per una variabile casuale continua la mediana è definita, tramite la condizione di normalizzazione, nella seguente maniera
$$
	\int\limits_{-\infty}^{\mu_{\frac{1}{2}}} p(x)dx = \int\limits_{\mu_{\frac{1}{2}}}^{+\infty} p(x)dx = \frac{1}{2}
$$
Per una variabile discreta non è detto che questo valore esista e sia univocamente determinato (proprio per questo la mediana è rilevante per le distribuzioni continue) e, proprietà degna di nota, è il fatto che se la funzione di distribuzione è simmetrica rispetto al valore medio, allora la media coincide con la mediana. Definiamo adesso la \index{moda}
\dfn{Moda}{La \textbf{moda}\index{moda} di una variabile casuale $x$ è il valore della variabile casuale (se \textbf{esiste} ed è \textbf{unico}) in corrispondenza del quale la funzione ha un massimo}. \\
Come caratterizziamo quanto si disperde la funzione di distribuzione attorno al valore medio? Si fa definendo una funzione  il cui valore di aspettazione definisce in media quanto si disperde rispetto al valore medio, dunque "pesiamo" la dispersione rispetto al valore medio con la probabilità che la variabile aleatoria assume quello specifico valore. \\
Che funzione possiamo prendere? Una funzione del tipo $f(x) = x - \mu$ non va bene siccome:
$$
	E[f(x)] = E[x-\mu] = E[x]-E[\mu] = \int\limits_{-\infty}^{+\infty} xp(x) - \int\limits_{-\infty}^{+\infty} \mu p(x)dx = \mu - \mu \int\limits_{-\infty}^{+\infty} p(x)dx = \mu - \mu = 0
$$
dunque questo valore di aspettazione non ci fornisce niente di utile, siccome le fluttuazioni statistiche attorno al valore medio tendono a compensarsi. Possiamo però pensare di utilizzare le fluttuazioni quadratiche $(x-\mu)^2$, dunque:
\begin{equation}
\sigma^2 = E[(x-\mu)^2] = \begin{cases} \sum\limits_{k} (x_k - \mu)^2 P(x_k) \\
\int\limits_{-\infty}^{+\infty} (x-\mu)^2 p(x)dx
 \end{cases}
\end{equation}
Definiamo dunque la variazione di una distribuzione come:
\dfn{Varianza di una distribuzione}{Si definisce \textbf{varianza}	\index{varianza} $\sigma^2$ di una distribuzione il valore di aspettazione di $(x-\mu)^2$, dunque:
\begin{equation}
	\sigma^2 = E[(x-\mu)^2]
\end{equation}
e definiamo la \textbf{deviazione standard}\index{deviazione standard} $\sigma$ come la radice quadrata della varianza
\begin{equation}
	\sigma = \sqrt{\sigma^2}
\end{equation}
}
\nt{La deviazione standard ha le stesse dimensioni fisiche della variabile casuale di partenza, dunque è la deviazione standard a caratterizzare la misura della dispersione attorno alla media cercata.}
\noindent Osserviamo una proprietà utile della varianza, ovvero che se $c$ è una costante, allora
$$
	\text{Var}(cx) = E[(cx - c\mu)^2] = E[c^2(x - \mu)^2] = c^2 \text{Var}(x) 
$$
Dimostriamo una formula equivalente per il calcolo della varianza
\begin{equation*}
	\sigma^2 = E[(x - \mu)?2] = E[x^2 - 2 \mu x + \mu^2] = E[x^2] - 2 \mu E[x] + E[\mu^2] = E[x^2] - 2 \mu^2 + \mu^2 = E[x^2] - \mu^2
\end{equation*}
dunque
\begin{equation}
	\sigma^2 = E[x^2] - \mu^2
	\label{calcolo_sigma}
\end{equation}
Un concetto utile che si applica alle funzioni di distribuzione di variabile continua (ma ha senso per lo più se si tratta di una distribuzione unimodale) è quello di semilarghezza a metà altezza, ovvero la distanza fra le ascisse $x_a$ e $x_b$ dei punti intersecati dalla retta orizzontale che interseca l'asse delle ordinate in corrispondenza della metà del valore della moda della distribuzione, ovvero il valore massimo assunto da essa. 
\dfn{FWHM e HWHM}{
La quantità
\begin{equation}
	\text{FWHM} = |x_b - x_a|
\end{equation}
prende il nome di {\it full width at half maximum}\index{FWHM}\ignorespaces , mentre la quantità 
\begin{equation}
	\text{HWHM} = \frac{|x_b - x_a|}{2}
\end{equation}
prende il nome di {\it half width at half maximum}\index{HWHM}\ignorespaces
}
\noindent La seconda quantità, ovvero la HWHM, è una stima abbastanza ragionevole, nella maggior parte delle distribuzioni, della deviazione standard, nel senso che
\begin{equation}
	\text{HWHM} = c \sigma
\end{equation}
con $c$ dell'ordine delle unità.
\nt{In un certo senso possiamo affermare che geometricamente la deviazione standard rappresenta una sorta di \emph{larghezza della distribuzione} che stiamo considerando, anche se la disuguaglianza di Chebyshev \index{disuguaglianza di Chebyshev}\ignorespaces che andremo a considerare fra poco lo renderà ancora più chiaro}
\thm{Disuguaglianza di Chebyshev\index{Disuguaglianza di Chebyshev}\ignorespaces}{Sia $x$ una variabile casuale tale che esistano finiti la media $\mu$ e la varianza $\sigma^2$; preso $c \in \mathbb{R}^+$ si ha che
\begin{equation}
	P(|x-\mu| \geq c\sigma) \leq \frac{1}{c^2} 
\end{equation}
}
\begin{myproof}
	senza perdita di generalità consideriamo $x$ come una variabile continua (sebbene la dimostrazione nel caso discreto è analoga).
\begin{align*}
\sigma^2 = \int\limits_{-\infty}^{+\infty} (x-\mu)^2 p(x)dx \geq \int\limits_{|x-\mu| \geq c\sigma} (x-\mu)^2 p(x)dx \geq \int\limits_{|x-\mu| \geq c\sigma} c^2 \sigma^2 p(x)dx = c^2 \sigma^2 \int\limits_{|x-\mu| \geq c\sigma} \geq c^2\sigma^2P(|x-\mu| \geq c\sigma)
\end{align*}
ma ciò implica la tesi, siccome
$$
	c^2 \cancel{\sigma^2} P(|x-\mu| \geq c\sigma) \leq \cancel{\sigma^2} \implies P(|x-\mu| \geq c\sigma) \leq \frac{1}{c^2}
$$
\end{myproof}

\pagebreak
\printindex
\end{document}
