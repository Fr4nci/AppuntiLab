\documentclass{report}
\input{preamble}
\input{macros}
\input{letterfonts}
\title{Appunti Lab}
\author{Francesco Sermi}
\date{}
\makeindex
\begin{document}
	\maketitle
	\tableofcontents
	\newpage
	\chapter{Introduzione}
	Il concetto di \emph{incertezza} e di \emph{misura} gioca un ruolo centrale nelle scienze sperimentali siccome noi \textbf{non} siamo mai in grado di misurare una grandezza fisica con un'accuratezza infinita, dunque il nostro risultato manca di una parte sostanziosa del proprio contenuto se non vi è assegnata una stima dell'incertezza compiuta nella misura effettuata. \\
	Il primo sforzo nella storia delle scienze verso un sistema standard di unità di misure è stato costituito, nella Francia del XVIII secolo, del sistema metrico decimale. Una domanda molto interessante che il lettore si potrebbe chiedere sarebbe la seguente \emph{"Come possiamo definire un'unità di misura?"} La risposta più semplice, ma non banale, è tramite dei campioni inalterabili e riproducibili: si riporta proprio la realizzazione del metro campione e del kilogrammo campione realizzati, sebbene subiscano una contaminazione, ogni anno, pari a $1 \si{\micro\gram}$. \\
	Oggi è stato definito il Sistema internazione delle misure che si basa sul fissare 7 unità di base da cui è possibile esprimere tutte le altre come combinazione delle altre (si rimanda la lettura delle dispense di Baldini per usufruire dei suoi grafici e/o tabelle). \\
	\section{L'errore massimo e le sue limitazioni}
	Si osserva che nella sua formulazione più elementare il concetto di \emph{errore massimo} è legato alla domanda \emph{"Qual è il più piccolo intervallo che contiene con certezza il valore numerico della quantità che sto misurando?"}. In altre parole, il risultato della misura di una generica grandezza fisica $x$ risulta essere pari a:
	\begin{equation}
		x = \hat{x} \pm \Delta x
		\label{err_max}
	\end{equation}
	intendo come $\Delta x$ l'errore massimo\index{errore massimo}\ignorespaces, ovvero stiamo dicendo che l'intervallo $[\hat{x} - \Delta x; \hat{x} + \Delta x]$ è il più piccolo intervallo possibile che ci dà la certezza di includere il valore incognito $x$. Definiamo adesso una serie di termini comodi per esprimere al meglio i concetti successivi:
	\begin{itemize}
		\item $x$ è il valore, incognito (siccome non lo conosciamo mai appieno), della grandezza che vogliamo misurare e che chiameremo \emph{misurando};
		\item $\hat{x}$ è la migliore stima di $x$ che possiamo fornire a partire dei dati a nostra disposizione (e che chiameremo \emph{valore centrale} o \emph{migliore stima} della nostra misura);
		\item $\Delta x$ è l'incertezza di misura e, in questo \textbf{caso}, coincide con l'\textbf{errore massimo}
	\end{itemize}
	Tuttavia, nel caso di misure ripetute, giungiamo ad un assurdo: infatti, se una misura fluttua, da un punto di vista operativo, non possiamo escludere che una nuova misura della grandezza non fornisca un valore al di fuori dell'intervallo iniziale di incertezza e, ove questo accade, siamo costretti ad allargare tale intervallo, giungendo, dunque, ad un evidente paradosso, siccome acquisire nuove informazioni può solo peggiore (o lasciare invariato, se otteniamo delle misure che fanno ancora parte di $[x-\Delta x; x + \Delta x]$) il nostro stato di conoscenza, almeno determinando l'incertezza come errore massimo. \\
	Questo è il motivo per cui non utilizziamo quasi mai il concetto di errore massimo, ma utilizzeremo il concetto di errore statistico.
	\begin{equation}
		x = \hat{x} \pm \sigma_x
		\label{err_stat}
	\end{equation}
	La \ref{err_stat} ha un significato diverso da \ref{err_max} prima siccome questa definisce un intervallo che non ci dà la \emph{certezza} ma solo la probabilità, ben definita, di contenere il valore del misurando. \\
	Adesso definiamo un altro modo molto utili per quantificare quanto è grosso, rispetto alla misura, l'errore che noi commettiamo:
	\dfn{Errore percentuale}{L'errore relativo\index{errore relativo} è il rapporto tra l'incertezza della misura e il suo valore centrale:
	\begin{equation}
		e_{\%} = \frac{\sigma_x}{|\hat{x}|}
\end{equation}}
	\section{Precisione vs accuratezza}
	E' molto sottile la distinzione fra la precisione di uno strumento e la sua accuratezza: con il primo termine si indica l'accordo tra il valore misurato dallo strumento e quello effettivo del misurando, mentre con il secondo si intende il grado di consistenza fra i risultati di misure successive della stessa quantità nelle medesime condizioni
	\chapter{Probabilità}
	Grazie al matematico Kolmogorov abbiamo la prima costruzione rigorosa della teoria della probabilità, in una struttura che, sostanzialmente, sopravvive ancora oggi nei manuali moderni. \\
	La struttura di base su cui si fonda la struttura assiomatica della probabilità parte dal definire lo spazio campionario
	\dfn{Spazio campionario $\Omega$}{Lo spazio campionario\index{spazio campionario} $\Omega$ è l'insieme (numerabile) di tutte le possibili realizzazioni elementari di un dato fenomeno e lo spazio degli eventi\index{spazio degli eventi} $\mathcal{F}$ l'insieme di tutti i sottoinsiemi di $\Omega$ tale che:
	$$
		\# \mathcal{F} = 2^{\# \Omega}
	$$}
\noindent L'idea di Kolmogorov è quella di definire la probabilità\index{probabilità} direttamente sullo spazio degli eventi-cioè possiamo assegnare una probabilità non solo ad ogni elemento dello spazio campionario, ma anche ad uno qualsiasi dei suoi sottoinsiemi.
	Definiamo adesso il concetto di probabilità:
	\dfn{Probabilità}{Definiamo probabilità una misura P su $\mathcal{F}$ che associ univocamente ad ogni elemento E di $\mathcal{F}$ un numero reale $P(E)$ che soddisfa le seguenti tre proprietà (o \textbf{assiomi di Kolmogorov}\index{assiomi di Kolmogorov}\ignorespaces):
	\begin{enumerate}[label=\protect\circled{\arabic*}]
		\item $0 \leq P(E) \leq 1 \, \forall E \in \mathcal{F}$
		\item $P(\Omega) = 1$
		\item $P(E_1 \cup E_2) = P(E_1) + P(E_2) \, \text{se} \, E_1 \cap E_2 = \emptyset$
	\end{enumerate}		
	}
\noindent Possiamo utilizzare il terzo assioma di Kolgomorov all'unione numerabili di eventi \emph{disgiunti} (ovvero eventi per cui la loro intersezione è nulla)
	\cor{Unione numerabili di eventi disgiunti}{$P(\bigcup\limits_{i}^n E_i) = \sum\limits_{i}^n P(E_i)$ se $E_1 = E_2 = \cdots = E_n = 0$}
	\begin{myproof}
	Si procede per induzione su $n$. Per $n = 1$ è banale, siccome:
	$$
		P \left( \bigcup_i^1 E_i \right) = P(E_1) = \sum_i^1 P(E_i) = P(E_i)
	$$
	Adesso mostriamo $n \implies n+1$:
	$$
		\sum_i^{n+1} P(E_i) = P(E_1) + \cdots + P(E_{n+1}) = \text{(ip. induttiva)} \, P \left(\bigcup_i^n E_i \right) + P(E_{n+1}) = P \left( \sum_{i}^{n+1} E_i \right)
	$$
	Siccome possiamo vedere nell'ultimo passaggio la somma fra due eventi che sono fra loro disgiunti, ovvero fra l'evento $E_1 \cup \E_2 \cdots \cup E_n$ e l'evento $E_{n+1}$ e per ipotesi sappiamo che $E_i \cap E_j = \emptyset \, \forall i,j$. \\
	La dimostrazione è dunque conclusa.
	\end{myproof}
	\cor{Probabilità complementare}{Dato un evento E e detto $\bar{E}$ il suo complementare in $\mathcal{F}$, si ha che:
		$$
			P(\bar{E}) = 1 - P(E)
		$$
		dove $P(\bar{E})$ è detta probabilità complementare\index{probabilità complementare} di $E$
	}
	\begin{myproof}
		Sapendo che $\bar{E} \cap E = \emptyset$ ma $\bar{E} \cup E = \Omega$:
		$$ 1 = P(\Omega) = P(\bar{E} + E) = P(\bar{E}) + P(E) \implies P(\bar{E}) = 1 - P(E)$$
	\end{myproof}
	\cor{Probabilità dell'insieme nullo}{$P(\emptyset) = 0$}
	\begin{myproof}
		$$
			P(\Omega) = P(\Omega \cup \emptyset)
		$$
		ma siccome $\Omega \cap \emptyset = \emptyset$ allora
		$$
			P(\Omega \cup \emptyset) = P(\Omega) + P(\emptyset) = P(\Omega) \implies 1 + P(\emptyset) = 1 \implies P(\emptyset) = 0
		$$
		La dimostrazione è dunque conclusa.
	\end{myproof}
	\cor{Limitatezza della probabilità di un sottoinsieme}{Se $E_1 \subset E_2 \implies P(E_1) \leq P(E_2)$}
	\begin{myproof}
		Se $E_1 \subset E_2 \implies E_2 = E_1 + (E_2 \setminus E_1)$ ma siccome $E_1 \cap (E_2 \setminus E_1) = \emptyset$:
		$$P(E_2) = P(E_1) + P(E_2 \setminus E_1) \implies P(E_1) \leq P(E_2)$$
	\end{myproof}
	\thm{Addizione delle probabilità}{Dati due eventi $E_1$ ed $E_2$, si ha che:
	\begin{equation}
		P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)
	\end{equation}			
	}
	\begin{myproof}
		Possiamo scrivere gli insiemi $E_1$ ed $E_2$ come unione di eventi disgiunti, infatti:
		$$
			E_2 = E_2 \cap \Omega = E_2 \cap (E_1 \cup \bar{E_1}) = (E_2 \cap E_1) \cup (E_2 \cap \bar{E_1})
		$$
		ma siccome $(E_2 \cap E_1) \cap (E_2 \cap \bar{E_1}) = \emptyset$ (altrimenti si giungerebbe ad un assurdo, visto che se $E_2 \cap E_1$ avesse degli elementi in comune con $E_2 \cap \bar{E_1}$ implicherebbe, visto che l'intersezione fra insieme è un'operazione che gode di proprietà associativa e commutativa, che $E_1 \cap \bar{E_1} \cap E_2 \cap E_2 \neq \emptyset$ ma $E_1 \cap \bar{E_1} = \emptyset$, dunque assurdo). Tornando alla dimostrazione:
		\begin{equation}
			P(E_2) = P \left( (E_2 \cap E_1) \cup (E_2 \cap \bar{E_1}) \right) = P(E_2 \cap E_1) + P(E_2 \cap \bar{E_1})
		\end{equation}
		d'altra parte abbiamo che
		$$
			E_1 \cup E_2 = (E_1 \cup E_2) \cap \Omega = (E_1 \cup E_2) \cap (E_1 \cup \bar{E_1}) = (E_1 \cap E_1) \cup (E_1 \cap \bar{E_1}) \cup (E_2 \cap E_1) \cup (E_2 \cap \bar{E_1})
		$$ = $E_1 \cup (E_2 \cap E_1) \cup (E_2 \cap \bar{E_1})$
		ma si osserva che il termine $E_1 \cup (E_2 \cap E_1)$ è ridondante, siccome $E_1 \cup (E_2 \cap E_1) = E_1$, dunque
		\begin{align*}
			E_1 \cup E_2 = E_1 \cup (E_2 \cup \bar{E_1}) \implies P(E_1) = P(E_1) + P(E_2 \cup \bar{E_1})
		\end{align*}
		e combinando le due relazioni ottenute
		\begin{equation*}			
			\begin{cases}
				P(E_2) = P(E_2 \cap E_1) + P(E_2 \cap \bar{E_1}) \\
				P(E_1 \cup E_2) = P(E_1) \cup P(E_2 \cap \bar{E_1})
			\end{cases}
		\end{equation*}
		dunque
		$$
			P(E_1 \cup E_2) = P(E_1) + P(E_2) - P(E_1 \cap E_2)
		$$
	\end{myproof}
	\section{Definizione operativa della probabilità}
	Il lettore più attento si sarà però accorto di come la definizione che abbiamo dato di probabilità non ci dà alcun modo con cui calcolare la probabilità, ma piuttosto una serie di assiomi da cui possiamo ricavare una serie di proprietà utili di cui gode la probabilità.
	\subsection{Definizione combinatoriale}
	Nella sua definizione combinatoriale, definiamo la probabilità di un evento E con quanto segue
	\dfn{Definizione combinatoriale della probabilità}{La probabilità di un evento E coincide con il rapporto tra il numero di casi favorevoli $n$ e il numero di casi possibili $N$, \underline{a condizione che questi siano tutti equiprobabili}
	\begin{equation}
		P(E) = \frac{n}{N}
\end{equation}		
	}
	\nt{Si osserva che questa definizione di probabilità rispetta i tre assiomi di Kolmogorov: il primo assioma discende dalla ovvia condizione per cui $0 \leq n \leq N$ ed il secondo fatto deriva dal fatto che se $n=N \implies P(E) = 1$. Per quanto riguarda la terza condizione, si osserva che se $E_1$ e $E_2$ sono due eventi disgiunti con rispettivamente $n_1$ e $n_2$ casi favorevoli, allora:
		\begin{equation*}
		P(E_1 \cup E_2) = \frac{n_1 + n_2}{N} = \frac{n_1}{N} + \frac{n_2}{N} = P(E_1) + P(E_2)
\end{equation*}		
	dunque anche il terzo assioma è rispettato	
	}
\noindent Tuttavia questa definizione operativa di probabilità possiede un grande problema: nella definizione è compiuto infatti un ragionamento circolare, siccome richiediamo l'equiprobabilità dei casi nella definizione stessa di probabilità.
	\subsection{Definizione frequentista}
	Quando è possibile ripetere un esperimento in condizioni controllate, è possibile definire la probabilità di un evento E come il limite della frequenza relativa all'evento stesso quando il numero di ripetizioni $N$ dell'esperimento tende all'infinito. Possiamo quindi pensare di effettuare un esperimento un numero $N$ arbitrariamente grande di volte, contare le $n$ volte in cui è avvenuto l'evento E e definire la probabilità $P(E)$ dell'evento come il limite del rapporto $\frac{n}{N}$
	\dfn{Definizione frequentista della probabilità}{La probabilità di un evento E si definisce come
	\begin{equation}
		P(E) = \lim_{N \to +\infty} \frac{n}{N}
	\end{equation}
	dove il limite va inteso nei termini della convergenza statistica, ovvero
	\begin{equation}
		\forall \epsilon, \delta > 0 \exists \tilde{N} > 0:\forall N, N>\tilde{N} \implies P \left( \left|\frac{n}{N} - P(E) \right| \geq \delta \right) \leq \epsilon
	\end{equation}
	}
	\nt{Il senso di questo limite, che va inteso come limite in senso statistico piuttosto che nel senso usuale dell'analisi matematica, è il fatto che non è possibile garantire a priori l'esistenza di un numero $N$ di ripetizioni del nostro esperimento che mi permetta di affermare con certezza che la differenza tra la frequenza registrata $\frac{n}{N} - P(E)$ sia minore di una certa quantità $\epsilon$: infatti, se effettuiamo due diverse serie di $N$ ripetizioni dell'esperimento otterrò frequenze relative $\frac{n}{N}$ diverse. Quello che possiamo però dire è il fatto che se $N$ è abbastanza grande allora posso rendere piccola a piacere la probabilità che $\frac{n}{N}$ si discosti da $P(E)$ di un valore prefissato $\delta$.}
	\ex{Lancio di un dado}{Se lanciamo $N$ volte un dado equo a sei facce e registriamo (al crescere di N) il numero $n$ di volte in cui esce, ad esempio, il numero 3, per $N$ molto grande il rapporto $\frac{n}{M}$ tenderà a $P(3) = \frac{1}{3}$ (nel senso della convergenza statistica)}
	\subsection{Definizione soggettivista della probabilità}
	\dfn{Definizione soggettivista}{La probabilità di un evento E si identifica con la misura del grado di fiducia che un individuo attribuisce al verificarsi di E, sulla base dell'informazione a sua disposizione}
	\nt{Il termine "soggettivo" si riferisce al fatto che persone diverse, sulla base di differenti informazioni, assoceranno, in generale, una probabilità diversa allo stesso evento e, proprio per questo fatto, si dice che questa definizione è \emph{soggettiva}}
\noindent Alla fine, sebbene questa definizione lasci inizialmente sbigottiti siccome si perde quell'oggettività che, in un certo senso, assicuravano le altre due definizioni, questa definizione rispetto di fatto come noi operiamo nella vita di tutti i giorni.
	All'interno della scuola soggettivista vi sono diversi approcci distinti per derivare le regole fondamentali della probabilità in un modo logicamente consistente (sebbene, con questo \emph{approccio}, gli assiomi non sono tali, ma regole che si ricavano da un principio più formale): il più popolare di questi approcci è il principio della \textbf{scommessa coerente}, il quale afferma che \emph{una volta assegnata la probabilità ad un evento dovremo essere disposti ad accettare scommesse sul verificarsi dell'evento stesso con un rapporto tra puntata e vincita determinato dalla probabilità stessa}\footnote{il senso è che se diciamo che due eventi sono equiprobabili allora dobbiamo essere pronti ad accettare scommesse 1:1, ovvero che una \emph{scommessa} è coerente se e solo se non determina \textbf{a priori} una perdita per il banco o per lo scommettitore, dunque è equivalente anche se i ruoli fossero scambiati} \\
	\section{Elementi di calcolo combinatorio}
	La combinatoria è quella branca della matematica che si occupa del \emph{contare}, dunque è strettamente connessa alla probabilità. \\
	Introduciamo il fattoriale\index{fattoriale}\ignorespaces di un numero nella seguente maniera
	\dfn{Fattoriale}{Dato un numero $n \in \mathbb{N}$, indichiamo con $n!$
		\begin{equation}
			n! = \prod_{k=1}^{n} k = n(n-1) \cdots 1	
		\end{equation}			
	}
\noindent Se ci pensiamo bene, la funzione fattoriale non è altro il numero di \emph{permutazioni}, ovvero il numero di modi in cui si possono disporre $n$ elementi se non possiamo ripeterli e \textbf{conta} l'ordine con cui questi elementi vengono disposti: supponiamo infatti di avere $20$ oggetti e di volerli disporre all'interno di un cassetto, noi abbiamo ben $20!$ modi possibili per metterli, siccome per il primo "posto" del cassetto possiamo metterci uno dei $20$, nel secondo "posto" $19$ oggetti e così via; ottenendo ben $20!$ fattoriale di possibilità. \\
	Le permutazioni possono essere viste come un caso particolare delle disposizioni, ovvero i modi con cui è possibile disporre $n$ oggetti in $k$ posti: per esempio, riprendendo l'esempio di prima, se noi volessimo prendere da $20$ oggetti $5$, presi a caso da questi $20$, all'interno di un cassetto in maniera tale che conti l'ordine con cui li mettiamo, si osserva che nel primo \emph{posto} del cassetto abbiamo $20$ oggetti disponibili, nel secondo $19$, nel terzo $18$, nel quarto $17$ e nell'ultimo $16$; dunque affermare che i modi totali sono $\frac{20!}{15!} = 20 \cdot 19 \cdot 18 \cdot 17 \cdot 16$. Definiamo quindi una disposizione come
	\dfn{Disposizione di $n$ elementi e di ordine $k$}{Definiamo una disposizione\index{disposizione} di $n$ elementi e di ordine $k$ come il numero di modi con cui è possibile disporre $n$ elementi in $k$ slot, pari a 		\begin{equation}
		D_{n,k} = \frac{n!}{(n-k)!}
	\end{equation}
}
\noindent Tornando alla funzione fattoriale, è comoda l'approssimazione di Stirling\index{approssimazione di Stirling}\ignorespaces (di cui non daremo una dimostrazione) per cui:
\begin{equation}
	n! = \sqrt{2\pi n} \left( \frac{n}{e} \right)^n
\end{equation}
Il numero di modi con cui è possibile scegliere  $k$ elementi non ordinati è dato dal coefficiente binomiale\index{coefficiente binomiale} $n$ su $k$
\dfn{Coefficiente binomiale $n$ su $k$}{Il coefficiente binomiale $n$ su $k$ è definito come
	\begin{equation}
		\binom{n}{k} = \frac{n!}{k!(n-k)!}
	\end{equation}
}
\cor{}{$$ \binom{n}{k} = \binom{n}{n-k} $$}
\begin{myproof}
	Banalmente, si osserva che
	$$
		\binom{n}{k} = \frac{n!}{k!(n-k)!} = \frac{n!}{(n-k)!(n-(n-k))!} = \binom{n}{n-k}
	$$
\end{myproof}
\noindent Il coefficiente binomiale è strettamente connesso al triangolo di Pascal (o di Newton) e può essere anche caratterizzato attraverso la formula della potenza del binomio (o teorema binomiale). Per farlo però ci servirà il seguente lemma:
\lemma{Proprietà fondamentale del coefficiente binomiale}{$$
	\binom{n+1}{k+1} = \binom{n}{k} + \binom{n}{k+1}
	$$
}
\begin{myproof}
	\begin{align*}
		&\binom{n}{k+1} + \binom{n}{k} = \frac{n!}{(k+1)!(n-k-1)!} + \frac{n!}{k!(n-k)!} = \frac{n!}{(k+1)k!(n-k-1)!} + \frac{n!}{k!(n-k)(n-k-1)!} \\ &= \frac{n!}{k!(n-k-1)!} \left( \frac{1}{k+1} + \frac{1}{n-k} \right) = \frac{n!}{k!(n-k-1)!} \left( \frac{n -k + k + 1}{(k+1)(n-k)} \right) = \frac{n!}{k!(n-k-1)!} \cdot \frac{(n+1)}{(k+1)(n-k)} \\ &= \frac{(n+1)!}{(k+1)!(n-k)!} = \binom{n+1}{k+1}
	\end{align*}
\end{myproof}
\thm{Teorema binomiale\index{teorema binomiale}\ignorespaces}{Lo sviluppo della potenza $n$-esima del binomio $x_1 + x_2$ è pari a
\begin{equation}
	(x_1 + x_2)^n = \sum_{k=0}^n \binom{n}{k} x_1^{n-k}x_2^k
\end{equation}
}
\begin{myproof}
si può procedere per induzione. Innanzitutto si osserva che, per $n=0$, si ha che
$$
	(x_1 + x_2)^0 = 1 = \sum_{k=0}^{0} \binom{n}{k} x_1^{n-k}x_2^k = \binom{0}{0} x_1^{0} x_2^0 = 1
$$
dunque per $n=0$ l'ipotesi è verificata. Mostriamo che $n \implies n+1$:
$$
	(x_1 + x_2)^{n+1} = (x_1 + x_2)^n (x_1 + x_2) = \sum_{k=0}^{n} \left[ x_1^{n-k}x_2^k \right] \cdot (x_1+x_2) = \sum_{k = 0}^{n} x_1^{n+1-k}x_2^k + \sum_{k = 0}^{n} x_1^{n-k}x_2^{k+1}
$$
Da qua soffermiamoci sul primo termine: si osserva che
$$
	\sum_{k=0}^{n} x_1^{n+1-k} x_2^k = \binom{n}{0} x_1^{n+1} + \sum_{k=1}^{n} \binom{n}{k} x_1^{n+1-k}x_2^k = \binom{n}{0} x_1^{n+1} + \sum_{k=0}^{n-1} \binom{n}{k+1} x_1^{n+1-k-1} x_2^{k+1} = x_1^{n+1} + \sum_{k = 0}^{n-1} \binom{n}{k+1} x_1^{n}x_2^{k+1}
$$
in cui si è semplicemente effettuato un cambio di variabile di variabile $k' = k + 1$ (ma siccome gli indici sono muti abbiamo semplicemente indicato $k'$ sempre con $k$). Adesso andiamo al secondo termine:
$$
	\sum_{k=0}^{n} \binom{n}{k} x_1^{n-k}x_2^{k+1} = \sum_{k=0}^{n-1} \binom{n}{k} x_1^{n-k}x_2^{k+1} + \binom{n}{n} x_2^{n+1} = \sum_{k=0}^{n-1} \binom{n}{k} x_1^{n-k}x_2^{k+1} + x_2^{n+1}
$$
Tornando dunque alla relazione iniziale, si deve avere che
$$
	(x_1 + x_2)^{n+1} = x_1^{n+1} + \sum_{k = 0}^{n-1} \binom{n}{k+1} x_1^{n}x_2^{k+1} + \sum_{k=0}^{n-1} \binom{n}{k} x_1^{n-k}x_2^{k+1} + x_2^{n+1} = x_1^{n+1} + \sum_{k=0}^{n-1} \left[ \binom{n}{k} + \binom{n}{k+1} \right] x_1^{n-k}x_2^{k+1} + x_2^{n+1}
$$
dunque, siccome per proprietà dei coefficienti binomiali si ha che $\binom{n+1}{k+1} = \binom{n}{k} + \binom{n}{k+1}$, allora
$$
	(x_1 + x_2)^{n+1} = x_1^{n+1} + \sum_{k=0}^{n-1} \binom{n+1}{k+1} x_1^{n-k}x_2^{k+1} + x_2^{n+1} = x_1^{n+1} + \sum_{k=1}^{n} \binom{n+1}{k} x_1^{n+1-k}x_2^{k} + x_2^{n+1} = \sum_{k=0}^{n+1} \binom{n+1}{k} x_1^{n+1-k}x_2^{k}
$$
Dunque la tesi è stata dimostrata 
\end{myproof}
\noindent Il coefficiente binomiale è comodo per calcolarsi le potenze del $2$ siccome se $x_1 = x_2 = 1$ si ha che
$$
2^n = \sum_{k = 0}^{n} \binom{n}{k} 1^{n-k} 1^{k} = \sum_{k=0}^{n} \binom{n}{k}
$$
Il coefficiente binomiale può anche essere generalizzato supponendo di voler dividere un insieme di $n$ elementi in $m$ sottoinsiemi disgiunti, ciascuno con un numero $k_i$ di elementi, la cui unione costituisca l'insieme di partenza, dunque si deve avere che $\sum_{i=1}^{m} k_i = n$
Il numero di modi con cui si può effettuare tale suddivisione prende il nome di \textbf{\index{coefficiente multinomiale} $n$ su $k_1, \dots , k_m$}  e si basa sull'idea che abbiamo $n$ su $k_1$ modi per scegliere il primo sottoinsieme, $n-k_1$ su $k_2$ modi per scegliere il secondo sottoinsieme e così via, dunque:
$$
	\binom{n}{k_1, k_2, \dots, k_m} = \binom{n}{k_1} \binom{n-k_1}{k_2} + \dots + \binom{n-k_1 \dots - k_{m-1}}{k_m}
$$
Sempre come per il coefficiente binomiale, possiamo caratterizzare il coefficiente multinomiale come lo sviluppo per la potenza $n$ di un numero arbitrario di monomi nella seguente maniera:
\begin{equation*}
	(x_1 + x_2 + \dots + x_m)^n = \sum_{ \{ (k_1, k_2, \dots, k_m) : \sum\limits_{i=1}^{m} k_i = n \} } \binom{n}{k_1, \dots k_m} x_1^{k_1} x_2^{k_2} \dots x_m^{k_m}
\end{equation*}
\section{Probabilità condizionata}
Tutto ciò che è stato mostrato nella sezione precedente può essere comodo per calcolare la probabilità di determinati eventi\footnote{usufruendo della definizione combinatoriale della probabilità} e consiglio caldamente di guardare gli esempi proposti da Baldini nel suo libro, su cui non mi soffermerò. Diamo delle definizioni
\dfn{Probabilità condizionata}{Dati due eventi $E_1$ ed $E_2$, con $P(E_2) \neq 0$, definiamo la \emph{probabilità condizionata}\index{probabilità condizionata} $P(E_1 | E_2)$ di $E_1$ dato $E_2$ (cioé la probabilità che si verifichi $E_1$ nel caso in cui sappiamo già che si verifica$E_2$) come
	\begin{equation}
		P(E_1 | E_2) = \frac{P(E_1 \cap E_2)}{P(E_2)}
	\end{equation}
}
\nt{La probabilità condizionata può essere vista come una sorta di "misura" dell'intersezione $E_1 \cap E_2$ pesata a $E_2$. E' possibile inoltre dimostrare che la probabilità condizionata soddisfa tutti gli assiomi della probabilità siccome ogni ogni evento $E$ può essere visto come una probabilità condizionata alla probabilità dello spazio campionario $P(E | \Omega)$}
\noindent L'utilità della probabilità condizionata è il fatto che possiamo calcolare la probabilità che due eventi $E_1$ ed $E_2$ si verifichino contemporaneamente nella seguente maniera:
$$
	P(E_1 \cap E_2) = P(E_1)P(E_2 | E_1) = P(E_2)P(E_1 | E_2)
$$
A questo punto diamo la definizione di eventi indipendenti \index{eventi indipendenti}
\dfn{Eventi indipendenti}{Si dice che due eventi $E_1$ ed $E_2$ sono \textbf{indipendenti} se il fatto che sia verificato $E_2$ non influenza la probabilità che si verifichi $E_1$, ovvero se
\begin{equation}
	P(E_1 | E_2) = P(E_1) \, \text{e} \, P(E_2 | E_1) = P(E_2)
\end{equation}
e per la definizione di probabilità condizionata possiamo riscrivere che
\begin{equation}
	P(E_1 \cap E_2) = P(E_1)P(E_2)
\end{equation}
}
\nt{Non bisogna confondere il concetto di \emph{\index{eventi indipendenti}} con il concetto di \emph{eventi disgiunti} e, dunque, incompatibili: si rimanda all'esempio di Baldini sul libro, in cui si osserva proprio il classico esempio del mazzo da carte in cui viene inserito un jolly. Prima dell'aggiunta del jolly i due eventi $E_1$="pesco un re" ed $E_2$="pesco una carta di cuori" erano indipendenti pure avendo un'intersezione non nulla, dopo l'aggiunta del jolly non sono più indipendenti ma non sono nemmeno incompatibili}
\section{Teorema di Bayes}
La naturale conseguenza di tutto ciò che abbiamo detto sulla probabilità condizionata è il teorema di Bayes\index{teorema di Bayes}, che lega tra loro la probabilità condizionata $P(E_1 | E_2)$ con la probabilità $P(E_2 | E_1)$
\thm{Teorema di Bayes}{La probabilità condizionata di $E_1$ dato $E_2$ è pari a
	\begin{equation}
		P(E_1 | E_2) = \frac{P(E_2 | E_1)P(E_1)}{P(E_2)}
	\end{equation}
dove $P(E_2 | E_1)$ è la probabilità condizionata di $E_2$ dato $E_1$
}
\begin{myproof}
	Dalla definizione di probabilità condizionata $P(E_1 | E_2)$ di $E_1$ dato $E_2$ sappiamo che
	$$
		P(E_1 | E_2) = \frac{P(E_1 \cap E_2)}{P(E_2)}
	$$
	con $P(E_2) \neq 0$, ma noi sappiamo pure che $P(E_1 \cap E_2) = P(E_2 | E_1)P(E_1)$, dunque si ottiene la tesi:
	$$
		P(E_1 | E_2) = \frac{P(E_2 | E_1)P(E_1)}{P(E_2)}
	$$
	La dimostrazione è dunque conclusa
\end{myproof}
\noindent Sfruttando inoltre il fatto che $E_1 \cup \bar{E_1} = \Omega$ allora $(E_1 \cap E_2) \cup (\bar{E_1} \cap E_2) = E_2 \cap (E_1 \cup \bar{E_1}) = E_2$, dunque:
$$
	P(E_2) = P(E_1 \cap E_2) + P(\bar{E_1} \cap E_2)
$$
(si ricorda che $(E_1 \cup E_2) \cup (\bar{E_1} \cup E_2) = \emptyset$ siccome se, per assurdo, $\exists c \in (E_1 \cap E_2) \cap (\bar{E_1} \cap E_2) \neq \emptyset \implies (c \in E_1 \cap E_2  \wedge c \in \bar{E_1} \cap E_2 )$ ma ciò implica che $(c \in E_1 \wedge c \in \bar{E_1})$ che è un assurdo; dunque possiamo usare il terzo assioma di Kolmogorov per stimare la probabilità di $E_2$). Ciò implica che
\begin{equation}
	P(E_1 | E_2) = \frac{P(E_2 | E_1)P(E_1)}{P(E_1 \cap E_2) + P(\bar{E_1} \cap E_2)}
\end{equation}
Più in generale, possiamo dire che se si dispone di un partizionamento dell'insieme $ \{ A_i \} $ (tali che $A_i \cap A_j = \emptyset \, \, \forall i, j, i \neq j$ e $\bigcup\limits_{i} A_i = \Omega$) allora possiamo generalizzare, per induzione su $i$, che $P(E_2) = \sum\limits_{i} P(E_2 | A_i)P(A_i)$, dunque:
\cor{Caratterizzazione di un evento tramite le partizioni}{Sia $\Omega$ lo spazio campionario degli eventi e sia $\{ E_i \}$ con $i \in I=[1, \dots, m], m \leq \#\mathcal{F}$ un partizionamento\footnote{ricordiamo che un partizione, per essere tale, deve risultare che $\forall i, j, i \neq j \, E_i \cap E_j = \emptyset$ e $\bigcup E_i = \mathcal{F}$} dell'insieme $\mathcal{F} = P(\Omega)$, ovvero lo spazio degli eventi, allora, dato un evento $A$, risulta che
$$
	A = (A \cap E_1) \cup (A \cap E_2) \dots (A \cap E_m)
$$
}
\begin{myproof}
	Si ha che $\bigcup\limits_i E_i = \mathcal{F}$ e siccome l'evento $A = A \cap \mathcal{F} = A \cap \bigcup\limits_i E_i = A \cap (E_1 \cup E_2 \dots E_m) = (A \cap E_1) \cup (A \cap E_2) \dots (A \cap E_m) = \bigcup\limits_i (A \cap E_i)$. Dimostriamo che gli $(A \cap E_i)$ sono tutti insiemi disgiunti $\forall i$: infatti se, per assurdo, $\exists c \in \mathcal{F} : \exists \tilde{i}, \tilde{d} \in I | (A \cap E_{\tilde{i}}) \cap (A \cap E_{\tilde{d}}) = c \neq \emptyset \implies (c \in (A \cap E_{\tilde{i}}) \wedge c \in (A \cap E_{\tilde{d}}))$ ma, per la definizione dell'operazione $\cap$, implica che $c \in E_{\tilde{i}} \wedge c \in E_{\tilde{d}}$ il che è un assurdo siccome gli $\{ E_i \}$ sono un partizionamento dell'insieme e, dunque, disgiunti.
	La dimostrazione è dunque conclusa
\end{myproof}
\noindent Andiamo adesso a scrivere il teorema di Bayes alla luce del corollario qua sopra
\begin{equation}
	P(A_1 | E_2) = \frac{P(E_2 | A_1)P(A_1)}{\sum\limits_{i} P(E_2 | A_i)P(A_i)}
\end{equation}
Questo teorema, sebbene risulti a prima vista banale, in realtà è molto importante siccome lega la probabilità diretta al problema di probabilità inversa. Si rimanda sempre al libro di Baldini per degli esempi per comprendere meglio il teorema di Bayes, sebbene sia utile osservare una cosa: mentre lo statista, oppure il matematico, studia la probabilità in astratto, ovvero cerca di calcolare la probabilità di un determinato processo "contando"; il fisico, di professione, cerca di inferire sul calcolo della probabilità attraverso delle raccolte dati, dunque tramite queste relazioni possiamo eventualmente ricondurci alla probabilità inversa, tuttavia cambia l'approccio "metodologico".
\section{Variabili casuali e funzioni di distribuzione}
Iniziamo questa sezione introducendo il concetto di variabile casuale
\dfn{Variabile casuale}{Una \emph{variabile casuale}\index{variabile casuale} o \emph{variabile aleatoria}\index{variabile aleatoria} è una variabile che rappresenta la realizzazione numerica di un processo causale, per cui il suo valore è soggetto a fluttuazioni casuali e non è noto a priori. Si distinguono in
\begin{itemize}
	\item \underline{discrete}, cioè variabili che possono assumere un numero finito o numerabile di valori;
	\item \underline{continue}, cioè variabili che possono assumere tutti i valori compresi in un intervallo.
\end{itemize}
}
\ex{}{\begin{itemize}
	\item L'uscita del lancio di un dado a sei facce è una variabile casuale discreta che può assumere esattamente sei valori: ${1, 2, 3, 4, 5, 6}$
	\item Il tempo necessario per arrivare da casa al luogo di lavoro è un esempio di variabile casuale continua
\end{itemize}}
\noindent Occupiamoci inizialmente di una variabile discreta $x$ che può assumere $n$ valori distinti $x_1, \dots, x_n$ e indichiamo con $P(x_k)$ la probabilità che assuma il valore $x_k$ e definiamo la \index{funzione di distribuzione}\footnote{in letteratura, si indica con questa espressione anche la funzione cumulativa di una distribuzione. Nel caso continuo, un termine che useremo sarà funzione di densità di probabilità} nella seguente maniera
\dfn{Funzione di distribuzione di $x$}{La \emph{funzione di densità di probabilità} è la funzione che associa ad ogni valore di $x_k$ della variabile la sua probabilità $P(x_k)$}
\noindent In questo contesto il secondo assioma di Kolmogorov si scrive nella forma di una \emph{condizione di normalizzazione}\index{condizione di normalizzazione}, che tutte le distribuzioni di distribuzione devono rispettare:
$$
	\sum_k P(x_k) = 1
$$
Nel caso di una variabile continua la definizione di funzione di distribuzione data per una variabile discreta fallisce clamorosamente siccome la probabilità che la variabile aleatoria $x$ assuma un valore \emph{esattamente} definito è zero (infatti mostreremo che si tratta di un integrale su un dominio di misura nulla). Ha però senso chiedersi qual è la probabilità, dato un punto generico $x_0$, che la variabile assuma un valore appartenente all'intervallo $[x_0, x_0 + dx]$
$$
	P(x_0, dx) = P(x_0 \leq x < x_0 + dx)
$$
Se a questo punto noi dividiamo per la larghezza dell'intervallo e consideriamo il limite per $dx \to 0$ otteniamo una sorta di probabilità specifica o probabilità specifica per unità di intervallo che chiamiamo \emph{\index{densità di probabilità}}:
\begin{equation}
	p(x_0) = \lim_{dx \to 0} \frac{P(x_0, dx)}{dx}
\end{equation}
che si tratta di una sorta di rapporto incrementale, per cui possiamo dire che la funzione di densità di probabilità è, in un certo senso, la derivata dalla funzione probabilità, dunque possiamo dire che la probabilità che la variabile aleatoria continua assuma un valore compreso nell'intervallo $[x_0, x_0 + dx]$ risulta essere pari
$$
	P(x_0, dx) = p(x_0)dx
$$
e, dunque, la probabilità che assuma un valore compreso nell'intervallo chiuso e limitato $[x_1, x_2]$ è pari a
\begin{equation}
	P(x_1, x_2) = \int_{x_1}^{x_2} p(x)dx
\end{equation}
In questo caso la \emph{condizione di normalizzazione}\index{condizione di normalizzazione}, nel caso di una variabile aleatoria continua, si scrive come
\begin{equation}
	\int_{-\infty}^{+\infty} p(x)dx = 1
\end{equation}
\nt{Si osserva che mentre la probabilità che la variabile casuale assuma un valore contenuto in $[x_0, x_0+dx]$ sia un numero adimensionale, nel caso della funzione di densità di probabilità questa dimensionalmente deve avere dimensione pari all'inverso di $x$}
\section{Valore di aspettazione, varianza e momenti di una distribuzione}
Nel momento in cui andiamo a fare un'indagine statistica o, nel caso nostro, effettuiamo un esperimento è comodo condensare le informazioni contenute nella funzione di distribuzione in una serie di parametri significativi come il valore che assume in media la nostra distribuzione oppure quanto la funzione di distribuzione si discosta in media da questo valore. \\
\noindent Andiamo, proprio per questo, a definire il valore di aspettazione, primo strumento utile per caratterizzare i parametri d'interesse di una distribuzione:
\dfn{Valore di aspettazione di $f(x)$}{Sia data una variabile aleatoria $x$ e una funzione $f(x)$, definiamo il valore di aspettazione\index{valore di aspettazione} come
\begin{equation}
	E[f(x)] = \begin{cases} \sum\limits_{k} f(x_k)P(x_k) \, \text{nel caso di variabili discrete} \\ \int_{-\infty}^{+\infty} f(x)p(x)dx \, \text{nel caso di variabili continue} \end{cases}
\end{equation}
}
\nt{Il valore di aspettazione, alla fine, non è altro che una sorta di media "pesata" (con la probabilità che la variabile casuale assuma il valore $x_k$) della funzione $f(x)$ calcolata in $x_k$}
\noindent Inoltre, il valore di aspettazione è un \emph{operatore lineare}, siccome
$$
	E[c_1 f(x) + c_2 g(x)] = \int_{-\infty}^{+\infty} (c_1 f(x) + c_2 g(x))dx = c_1 \int_{-\infty}^{+\infty} f(x)dx + c_2 \int_{-\infty}^{+\infty} g(x)dx = c_1 E[f(x)] + c_2 E[g(x)]
$$
Inoltre il valore di aspettazione di una costante è pari alla costante stessa, siccome
$$
	E[c] = \int_{-\infty}^{+\infty} cdx = c\int_{-\infty}^{+\infty} dx = c \, \text{per la condizione di normalizzazione}
$$
Definiamo a questo punto il valore medio di una variabile casuale $x$ (continua o discreta) come
\dfn{Valore medio di una variabile causale $x$}{Il valore medio\index{valore medio} di una variabile casuale $x$ si definisce come il valore di aspettazione di $x$
\begin{equation}
	\mu = E[x] = \begin{cases} \sum\limits_{k} x_kP(x_k) \\ \int_{-\infty}^{+\infty} xp(x)dx \end{cases}
\end{equation}
}
\noindent Naturalmente, se $c$ è una costante allora
$$
	E[cx] = cE[x]
$$
per linearità dell'integrale.
\nt{Nel caso particolare di una variabile casuale e discreta per cui si abbiano $n$ uscite equiprobabili $x_k$ equiprobabili (ovvero $P(x_1) = P(x_2) = \dots = P(x_n) = \frac{1}{n}$ si ha che
$$
	\mu = \sum_{k = 1}^{n} x_k P(x_k) = \frac{1}{n}\sum_{k=1}^{n} P(x_k)
$$
}
\noindent Tuttavia il valore medio non è l'unica stima possibile di tendenza centrale: possiamo definire anche la \emph{\index{mediana}}:
\dfn{Mediana}{Si definisce \textbf{mediana}\index{mediana} di una distribuzione quel valore $\mu_{\frac{1}{2}}$ della variabile casuale tale che
$$
	P(x \leq \mu_{\frac{1}{2}}) = P(x \geq \mu_{\frac{1}{2}})
$$
}
\noindent Per una variabile casuale continua la mediana è definita, tramite la condizione di normalizzazione, nella seguente maniera
$$
	\int\limits_{-\infty}^{\mu_{\frac{1}{2}}} p(x)dx = \int\limits_{\mu_{\frac{1}{2}}}^{+\infty} p(x)dx = \frac{1}{2}
$$
Per una variabile discreta non è detto che questo valore esista e sia univocamente determinato (proprio per questo la mediana è rilevante per le distribuzioni continue) e, proprietà degna di nota, è il fatto che se la funzione di distribuzione è simmetrica rispetto al valore medio, allora la media coincide con la mediana. Definiamo adesso la \emph{moda}\index{moda} di una distribuzione
\dfn{Moda}{La \textbf{moda} di una variabile casuale $x$ è il valore della variabile casuale (se \textbf{esiste} ed è \textbf{unico}) in corrispondenza del quale la funzione ha un massimo}. \\
Come caratterizziamo quanto si disperde la funzione di distribuzione attorno al valore medio? Si fa definendo una funzione  il cui valore di aspettazione definisce in media quanto si disperde rispetto al valore medio, dunque "pesiamo" la dispersione rispetto al valore medio con la probabilità che la variabile aleatoria assume quello specifico valore. \\
Che funzione possiamo prendere? Una funzione del tipo $f(x) = x - \mu$ non va bene siccome:
$$
	E[f(x)] = E[x-\mu] = E[x]-E[\mu] = \int\limits_{-\infty}^{+\infty} xp(x) - \int\limits_{-\infty}^{+\infty} \mu p(x)dx = \mu - \mu \int\limits_{-\infty}^{+\infty} p(x)dx = \mu - \mu = 0
$$
dunque questo valore di aspettazione non ci fornisce niente di utile, siccome le fluttuazioni statistiche attorno al valore medio tendono a compensarsi. Possiamo però pensare di utilizzare le fluttuazioni quadratiche $(x-\mu)^2$, dunque:
\begin{equation}
\sigma^2 = E[(x-\mu)^2] = \begin{cases} \sum\limits_{k} (x_k - \mu)^2 P(x_k) \\
\int\limits_{-\infty}^{+\infty} (x-\mu)^2 p(x)dx
 \end{cases}
\end{equation}
Definiamo dunque la variazione di una distribuzione come:
\dfn{Varianza di una distribuzione}{Si definisce \textbf{varianza}	\index{varianza} $\sigma^2$ di una distribuzione il valore di aspettazione di $(x-\mu)^2$, dunque:
\begin{equation}
	\sigma^2 = E[(x-\mu)^2]
\end{equation}
e definiamo la \textbf{deviazione standard}\index{deviazione standard} $\sigma$ come la radice quadrata della varianza
\begin{equation}
	\sigma = \sqrt{\sigma^2}
\end{equation}
}
\nt{La deviazione standard ha le stesse dimensioni fisiche della variabile casuale di partenza, dunque è la deviazione standard a caratterizzare la misura della dispersione attorno alla media cercata.}
\noindent Osserviamo una proprietà utile della varianza, ovvero che se $c$ è una costante, allora
$$
	\text{Var}(cx) = E[(cx - c\mu)^2] = E[c^2(x - \mu)^2] = c^2 \text{Var}(x) 
$$
Dimostriamo una formula equivalente per il calcolo della varianza
\begin{equation*}
	\sigma^2 = E[(x - \mu)?2] = E[x^2 - 2 \mu x + \mu^2] = E[x^2] - 2 \mu E[x] + E[\mu^2] = E[x^2] - 2 \mu^2 + \mu^2 = E[x^2] - \mu^2
\end{equation*}
dunque
\begin{equation}
	\sigma^2 = E[x^2] - \mu^2
	\label{calcolo_sigma}
\end{equation}
Un concetto utile che si applica alle funzioni di distribuzione di variabile continua (ma ha senso per lo più se si tratta di una distribuzione unimodale) è quello di semilarghezza a metà altezza, ovvero la distanza fra le ascisse $x_a$ e $x_b$ dei punti intersecati dalla retta orizzontale che interseca l'asse delle ordinate in corrispondenza della metà del valore della moda della distribuzione, ovvero il valore massimo assunto da essa. 
\dfn{FWHM e HWHM}{
La quantità
\begin{equation}
	\text{FWHM} = |x_b - x_a|
\end{equation}
prende il nome di {\it full width at half maximum}\index{FWHM}\ignorespaces , mentre la quantità 
\begin{equation}
	\text{HWHM} = \frac{|x_b - x_a|}{2}
\end{equation}
prende il nome di {\it half width at half maximum}\index{HWHM}\ignorespaces
}
\noindent La seconda quantità, ovvero la HWHM, è una stima abbastanza ragionevole, nella maggior parte delle distribuzioni, della deviazione standard, nel senso che
\begin{equation}
	\text{HWHM} = c \sigma
\end{equation}
con $c$ dell'ordine delle unità.
\nt{In un certo senso possiamo affermare che geometricamente la deviazione standard rappresenta una sorta di \emph{larghezza della distribuzione} che stiamo considerando, anche se la \emph{disuguaglianza di Chebyshevv} che andremo a considerare fra poco lo renderà ancora più chiaro}
\thm{Disuguaglianza di Chebyshev\index{disuguaglianza di Chebyshev}\ignorespaces}{Sia $x$ una variabile casuale tale che esistano finiti la media $\mu$ e la varianza $\sigma^2$; preso $c \in \mathbb{R}^+$ si ha che
\begin{equation}
	P(|x-\mu| \geq c\sigma) \leq \frac{1}{c^2} 
\end{equation}
}
\begin{myproof}
	senza perdita di generalità consideriamo $x$ come una variabile continua (sebbene la dimostrazione nel caso discreto è analoga).
\begin{align*}
\sigma^2 = \int\limits_{-\infty}^{+\infty} (x-\mu)^2 p(x)dx \geq \int\limits_{|x-\mu| \geq c\sigma} (x-\mu)^2 p(x)dx \geq \int\limits_{|x-\mu| \geq c\sigma} c^2 \sigma^2 p(x)dx = c^2 \sigma^2 \int\limits_{|x-\mu| \geq c\sigma} \geq c^2\sigma^2P(|x-\mu| \geq c\sigma)
\end{align*}
ma ciò implica la tesi, siccome
$$
	c^2 \cancel{\sigma^2} P(|x-\mu| \geq c\sigma) \leq \cancel{\sigma^2} \implies P(|x-\mu| \geq c\sigma) \leq \frac{1}{c^2}
$$
\end{myproof}
\section{Momenti di una distribuzione}
Generalizziamo alcune definizioni che abbiamo dato su alcuni parametri di una distribuzione introducendo il concetto di \emph{momento di ordine $n$}\index{momento di ordine $n$} di una variabile casuale $x$ attorno ad un punto $x_0$
\dfn{Momento di ordine $n$ attorno al punto $x_0$}{Il momento di ordine $n$ di una variabile casuale $x$ attorno al punto $x_0$ si definisce come il valore di aspettazione di $f(x) = (x-x_0)^n$, dunque
\begin{equation}
	\mathcal{M}_n(x_0) = E[(x-x_0)^n] = \begin{cases} \sum\limits_{k} (x_k - x_0)^n P(x_k) \\ \int\limits_{-\infty}^{+\infty} (x-x_0)^n p(x)dx	\end{cases}
\end{equation}
}
\noindent Hanno rilevanza particolare i \underline{momenti algebrici}\index{momento algebrico} $\lambda_n$, ossia i momenti di ordine generico attorno al punto $x_0 = 0$
$$
	\lambda_n = \mathcal{M}_n(0)
$$
ed i \underline{momenti centrali}\index{momento centrale} $\mu_n$, ossia i momenti di ordine generico attorno al valore medio $\mu$ di $x$
$$
	\mu_n = \mathcal{M}_n(\mu)
$$
dunque possiamo dire che il valor medio\index{valore medio} è il momento algebrico di ordine 1 e la varianza\index{varianza} invece è il momento centrale di ordine 2. Oltre al valor medio e alla varianza, sono utili i momenti centrali di ordine 3 siccome sono i primi momenti di ordine dispari a non annullarsi e poiché misurano l'eventuale asimmetria della funzione di distribuzione, pesando con il segno le code a destra e a sinistra della media. Troviamo una forma più agevole per calcolare $\mu_3$
$$
	\mu_3 = E[(x-\mu)^3] = E [ x^3 - \mu^3 -3x^2 \mu + 3x \mu^2 ] \implies E[x^3] - 3\mu E[x^2] + 3\mu^2 E[x] - \mu^3
$$
e siccome $\sigma^2 = E[x^2] - \mu^2 \implies E[x^2] = \sigma^2 + \mu^2$ allora
$$
	\mu_3 = E[x^3] - 3 \mu (\sigma^2 + \mu^2) + 3 \mu^3 - \mu^3 = E[x^3] - 3 \sigma^2 \mu - \mu^3
$$
A questo punto, definiamo il \emph{coefficiente di asimmetria} $\gamma_1$\index{coefficiente di asimmetria}
\dfn{Coefficiente di asimmetria $\gamma_1$}{Il coefficiente di asimmetria si definisce come il rapporto fra il momento algebrico di ordine $n=3$ ($\mu_3$) e la deviazione standard al cubi ($\sigma^3$)
\begin{equation}
	\gamma_1 = \frac{\mu_3}{\sigma^3}
\end{equation}
e si tratta di una quantità adimensionale che vale zero per le distribuzioni simmetriche rispetto al valore medio e che è diversa da zero se la funzione di distribuzione presenta una coda più lunga dell'altra: nel caso in cui di $\gamma_1 > 0 \implies$ la coda a destra è più lunga.
}
\section{Funzione cumulativa}
Data una variabile casuale $x$ la \emph{funzione cumulativa} è definita come
\dfn{Funzione cumulativa}{La \textbf{funzione cumulativa}\index{funzione cumulativa} è definita come
\begin{equation}
	F(x') = P(x \leq x')
\end{equation}
e si indica solitamente con lo stesso nome della funzione di distribuzione siccome ha il suo stesso dominio. In maniera operativa, possiamo affermare che
\begin{equation}
	F(x) = \begin{cases} \sum\limits_{x_k \leq x} P(x_k) \\ \int\limits_{-\infty}^{x} p(t)dt \end{cases}
\end{equation}
}
\noindent La funzione cumulativa è una funzione \textbf{monotona crescente} e, per la condizione di normalizzazione, si deve avere che
\begin{align*}
	&\lim_{x \to -\infty} F(x) = 0 & &\lim_{x \to +\infty} F(x) = 1
\end{align*}
Inoltre, siccome è strettamente crescente e continua, è invertibile, dunque esiste uno ed un solo valore di $x$ per cui $F(x) = q$. E' possibile quindi definire un inverso della funzione cumulativa che viene chiamata \emph{funzione di distribuzione inversa}
\section{Variabili multi-variate}
La nostra discussione, fino ad adesso, si è concentrata sulla variabili casuali singolo. Come possiamo caratterizzare un insieme di variabili casuali $x_1, \dots, x_n$? In maniera più banale di quanto si creda, si potrebbe pensare di considera la funzione di distribuzione congiunta che, ad ogni punto del polirettangolo $A_1 \cup A_2 \cup \dots \cup A_n$, associa la probabilità che le variabili assumano quel \emph{set} di valori. \\
Consideriamo, per semplicità, due variabili casuali continue $x_1$ e $x_2$ descritta dalla densità di probabilità congiunta $p(x_1, x_2)$ tale che
\begin{equation}
	p(x_1, x_2) \geq 0 \wedge \int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} p(x_1, x_2) dx_1dx_2 = 1
\end{equation}
La probabilità che la coppia ordinata $(x_1, x_2) \subset A$ è pari a
\begin{equation}
	\iint_A p(x_1, x_2) dx_1dx_2
\end{equation}
e possiamo sempre definire il valore di aspettazione per una generica funzione $f(x_1, x_2)$ come
\begin{equation}
	E[f(x_1, x_2)] = \int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} f(x_1, x_2)p(x_1, x_2)dx_1dx_2
\end{equation}
Una domanda che ci potremmo porre è la seguente: come posso capire se due o più variabili $x_1, \dots, x_m$ sono \emph{indipendenti}? Si potrebbe pensare che questa caratteristica si dovrebbe, in qualche maniera, andare a "rintracciare" dalla funzione di densità di probabilità congiunta. \\ Per fare ciò potremmo pensare di far variare il valore di una variabile aleatoria (come ad esempio $x_1$ e di fissare quello delle altre variabili. Per esempio, nel caso di due variabili, $x_1$ e $x_2$, allora potremmo pensare di definire una funzione di densità di probabilità \emph{condizionata}, ovvero
\begin{align*}
	&p(x_1 | x_2 \, \text{fissato}) = p_1(x_1) & &p(x_2 | x_1 \, \text{fissato})
\end{align*}
Tuttavia come possiamo scrivere questa probabilità? Si potrebbe pensare che, in maniera abbastanza banale, questa probabilità condizionata si ottiene prendendo la funzione di densità di probabilità e fissando l'altra variabile. Tuttavia, come mostrano molto bene le figure e gli esempi riportati da Baldini (che consiglio di vedere), questa "forma" non è correttamente normalizzata; ma non tutto è da buttare, siccome basta dividere la densità di probabilità per un valore costante ovvero $\int\limits_{-\infty}^{+\infty} p(x_1, x_2)dx_1$:
\begin{equation*}
	p(x_1 | x_2) = \frac{p(x_1, x_2)}{ \int\limits_{-\infty}^{+\infty} p(x_1, x_2) dx_1 }
\end{equation*}

\dfn{Probabilità condizionata di due variabili dipendenti\index{probabilità condizionata}}{La \textbf{probabilità condizionata} di $x_1$ relativamente a $x_2$ (fissato) è definita come
\begin{equation} \label{eq:dipend_norm}
	p(x_1 | x_2) = \frac{p(x_1, x_2)}{\int\limits_{-\infty}^{+\infty} p(x_1, x_2)dx_1}
\end{equation}
Definiamo \textbf{densità di probabilità marginale}\index{densità di probabilità marginale} il denominatore della precedente formula (siccome dopo l'integrazione sarà esclusivamente in funzione di $x_2$ e non più di $x_1$)
\begin{equation}
	p_2(x_2) = \int_{-\infty}^{+\infty} p(x_1, x_2) dx_1
\end{equation}
(per la probabilità condizionata di $x_2$ relativo a $x_1$ e la corrispondente probabilità marginale basta sostituire $x_1$ al posto di $x_2$ e viceversa nelle precedenti relazioni).
}
\nt{Si può dimostrare che la forma~\ref{eq:dipend_norm} è correttamente normalizzata, siccome
\begin{align*}
	&p(x_1 | x_2 \, \text{fissato}) = \frac{p(x_1, x_2)}{\int\limits_{-\infty}^{+\infty} p(x_1, x_2 \, \text{fissato})dx_1} \implies \int\limits_{-\infty}^{+\infty} p(x_1 | x_2)dx_1 = \int\limits_{-\infty}^{+\infty} \frac{p(x_1, x_2)}{\int\limits_{-\infty}^{+\infty} p(x_1, x_2)dx_1} dx_1 \\ &= \frac{1}{\int\limits_{-\infty}^{+\infty} p(x_1, x_2)dx_1} \int\limits_{-\infty}^{+\infty} p(x_1, x_2)dx_1 \implies p(x_1 | x_2) = 1
\end{align*}
ed è ragionevole portare fuori l'integrale $p_2(x_2) = \int\limits_{-\infty}^{+\infty} p(x_1, x_2)dx_1$ dall'integrale siccome la densità di probabilità marginale non è più in funzione della variabile $x_2$ dunque si comporta come una costante rispetto a $x_1$ 
}
\noindent In maniera molto simile come per le distribuzioni univariate, diciamo che due variabili aleatorie $x_1$ e $x_2$ sono indipendenti se la densità di probabilità congiunta può essere fattorizzata come il prodotto delle due densità di probabilità marginali, ovvero
\dfn{Indipendenza statistica di due variabili aleatorie}{\begin{equation}
	p(x_1, x_2) = p_1(x_1)p_2(x_2)	
\end{equation}
}
\noindent Dunque, rimettendo insieme con ciò che eravamo partiti, allora possiamo dire che
\begin{align*}
	&p(x_1 | x_2) = \frac{p_1(x_1)p_2(x_2)}{p_2(x_2)} = p_1(x_1) & &p(x_2 | x_1) = \frac{p_1(x_1)p_2(x_2)}{p_1(x_1)} = p_2(x_2)
\end{align*}
Una proprietà molto comoda delle variabili aleatorie è il fatto che, se due variabili $x_1$ e $x_2$ (ci restringiamo al caso in due variabili, ma ciò non è restrittivo) sono \textbf{indipendenti}, allora si dimostra, in maniera banale, che il valore di aspettazione del loro prodotto è uguale al prodotto dei valori di aspettazione, infatti:
$$
E[x_1 x_2] = \int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} x_1 x_2 p(x_1, x_2)dx_1 dx_2 = \int\limits_{-\infty}^{+\infty} \int\limits_{-\infty}^{+\infty} x_1 x_2 p_1(x_1)p_2(x_2)dx_1 dx_2
$$ 
è possibile applicare il teorema di Fubini, da cui
$$
E[x_1 x_2] = \int\limits_{-\infty}^{+\infty} dx_1 \int\limits_{-\infty}^{+\infty} x_1 p_1(x_1) x_2 p_2(x_2) dx_1dx_2 = \int\limits_{-\infty}^{+\infty} x_1 p_1(x_1) dx_1 \int\limits_{-\infty}^{+\infty} x_2 p_2(x_2) dx_2
$$
(nell'ultimo passaggio abbiamo portato fuori dei termini che sono solamente in funzione di $x_1$ e dunque si comportano come una costante rispetto a $x_2$). \\
\dfn{Covarianza\index{covarianza}}{Date due variabili casuali $x_1$ e $x_2$ e dette $\mu_1$ e $\mu_2$ il momento algebrico\index{momento algebrico} di ordine 1 (ovvero sono il valore medio\index{valore medio} delle due distribuzioni) definiamo la covarianza $\text{Cov}(x_1, x_2)$ o $\sigma_{x_1x_2}$ come il valore di aspettazione delle relative fluttuazioni attorno al valore medio
\begin{equation} \label{eq:cov}
	\text{Cov}(x_1, x_2) = \sigma_{x_1 x_2} = E[(x_1 - \mu_1)(x_2 - \mu_2)]
\end{equation}
}
\noindent Si può dimostrare banalmente che la \ref{eq:cov} può essere scritta come
$$
	\text{Cov}(x_1, x_2) = E[x_1 x_2] - \mu_2 E[x_1] - \mu_1 E[x_2] + \mu_1 \mu_2 = E[x_1 x_2] - E[x_1] E[x_2]
$$
dunque, si ha che se $E[x_1 x_2] = E[x_1] E[x_2] = 0 \iff \text{Cov}(x_1, x_2) = 0$. Dunque si osserva che, se due variabili aleatorie sono indipendenti, allora $\text{Cov}(x_1, x_2) = 0$ ma non è vero il contrario (riporto sotto l'esempio del Baldini).
\ex{Covarianza nulla, variabili dipendenti}{Consideriamo una variabile casuale continua $x$ con una funzione di distribuzione $p(x)$ simmetrica rispetto a $0$, il che implica che tutti i momenti algebrici di ordine dispari sono nulli. Si ha banalmente
$$
	\text{Cov}(x, x^2) = E[x^3] - E[x]E[x^2] = 0
$$
siccome $E[x]$ e $E[x^3]$ sono nulli, tuttavia è ovvio che le due variabili non sia indipendenti statisticamente
}
\noindent Da un punto di vista matematico, possiamo dire che la covarianza è una \textbf{forma bilineare simmetrica}, nel senso che gode delle seguenti proprietà:
\begin{enumerate}[label=\protect\circled{\arabic*}]
	\item $\text{Cov}(x_1, x_2) = \text{Cov}(x_2, x_1)$
	\item $\text{Cov}(c_1x_1 + c_2x_2, x_3) = c_1\text{Cov}(x_1, x_3) + c_2\text{Cov}(x_2, x_3)$
	\item $\text{Cov}(x_1, c_2x_2 + c_3x_3) = c_2\text{Cov}(x_1, x_2) + c_3\text{Cov}(x_1, x_3)$
\end{enumerate}
\noindent \begin{myproof} per la \circled{1} si osserva banalmente che
$$
	\text{Cov}(x_1, x_2) = E[x_1 x_2] - E[x_1]E[x_2] = E[x_2 x_1] - E[x_2]E[x_1] = \text{Cov}(x_2, x_1)
$$
Per la \circled{2} si osserva che
\begin{align*}
	&\text{Cov}(c_1 x_1 + c_3 x_3 , x_2) = E[(c_1 x_1 + c_3 x_3) x_2] - E[c_1 x_1 + c_3 x_3]E[x_2] = E[c_1 x_1 x_2] + E[ c_3 x_3 x_2] - E[c_1 x_1]E[x_2] - E[c_3 x_3]E[x_2] \\ 
	&= c_1E[x_1x_2] - c_1E[x_1]E[x_2] + c_3E[x_3x_2] - c_3 E[x_3]E[x_2] = c_1 \text{Cov}(x_1, x_2) + c_3 \text{Cov}(x_3, x_2)
\end{align*}
mentre per la \circled{3} si osserva che, usando le due proprietà dimostrate prima
\begin{align*}
	&\text{Cov}(x_1, c_2x_2 + c_3 x_3) = \text{Cov}(c_2x_2 + c_3x_3, x_1) = c_2 \text{Cov}(x_2, x_1) + c_3 \text{Cov}(x_3, x_1) = c_2 \text{Cov}(x_1, x_2) + c_3 \text{Cov}(x_1, x_3)
\end{align*}
\end{myproof}
\noindent Una proprietà minore è il fatto che $\text{Cov}(x_1, c) = 0$ con $c$ costante e, inoltre, possiamo definire la \emph{varianza}\index{varianza} di una variabile aleatoria come la covarianza della variabile con sé stessa, dunque
\begin{equation}
	\text{Cov}(x, x) = \text{Var}(x)
\end{equation}
\dfn{Matrice di covarianza\index{matrice di covarianza}}{La matrice di covarianza è la matrice simmetrica $n \times n$ che organizza le covarianze $\text{Cov}(x_i, x_j)$ di $n$ variabili
\begin{align}
	\Sigma =  \begin{bmatrix}
		\text{Cov}(x_1, x_1) & \text{Cov}(x_1, x_2) & \cdots & \text{Cov}(x_1, x_n) \\
		\text{Cov}(x_2, x_1) & \text{Cov}(x_2, x_2) & \cdots & \text{Cov}(x_2, x_n) \\
		\vdots & \vdots & \ddots & \vdots \\
		\text{Cov}(x_n, x_1) & \text{Cov}(x_n, x_2) & \cdots & \text{Cov}(x_n, x_n)
	\end{bmatrix}
\end{align}
}
\dfn{Correlazione}{
La correlazione\index{correlazione} è una versione \emph{riscalata} della covarianza
\begin{equation}
	\text{Corr}(x_1, x_2) = \rho_{x_1 x_2} = \frac{\text{Cov}(x_1, x_2)}{\sigma_1 \sigma_2}
\end{equation}
e possiede tutte le "proprietà" della matrice di covarianza.
}
\nt{La correlazione misura quanto sono \emph{correlate} fra loro due variabili aleatorie, siccome la correlazione assume tutti i valori compresi fra $1$ e $-1$. Formalmente, si può dimostrare che la correlazione agisce come una sorta di prodotto scalare mentre la norma come una norma, pertanto questa proprietà (ovvero quella che assume i valori compresi in $[-1;1]$ deriva da Cauchy-Schwartz)}
\noindent Due variabili $x_1$ e $x_2$ sono \emph{scorrelate} se $\text{Corr}(x_1, x_2) = 0$, altrimenti diciamo che se $\text{Corr}(x_1, x_2) > 0 \implies x_1, x_2$ sono \emph{positivamente correlate} altrimenti sono \emph{negativamente correlate}. Assume il valore di $1$ (o $-1$) quando dipendono linearmente una dall'altra: infatti se
$$
	x_2 = mx_1 + q \implies \text{Corr}(x_1, x_2) = \frac{\text{Cov}(x_1, x_2)}{\sigma_{x_1}\sigma_{x_2}} = \frac{\text{Cov}(x_1, mx_1 + q)}{\sigma_{x_1}\sigma_{mx_1 + q}}
$$
tuttavia si ha che $\sigma_{x_2} = \sqrt{E[(mx_1 + 	q - m\mu - q)^2]} = |m|\sigma_1$, dunque
$$
	\text{Corr}(x_1, x_2) = \frac{\text{Cov}(x_1, mx_1 + q)}{|m|\sigma_{x_1}^2} = \frac{\text{Cov}(x_1, mx_1) + \text{Cov}(x_1, q)}{|m|\sigma_{x_1}^2} = \frac{m\sigma_{x_1}^2}{|m|\sigma_{x_1}^2} = \frac{m}{|m|} = \pm 1
$$
Analogamente alla covarianza, si definisce anche una \emph{matrice di correlazione}\index{matrice di correlazione} come
\dfn{Matrice di correlazione}{
\begin{align}
\mathcal{R} = \begin{bmatrix}
	\text{Corr}(x_1, x_1) & \text{Corr}(x_1, x_2) & \ldots & \text{Corr}(x_1, x_n) \\
	\text{Corr}(x_2, x_1) & \text{Corr}(x_2, x_2) & \ldots & \text{Corr}(x_2, x_n) \\
	\vdots & \vdots & \ddots & \vdots \\
	\text{Corr}(x_n, x_1) & \text{Corr}(x_n, x_2) & \ldots & \text{Corr}(x_n, x_n)
\end{bmatrix} = \begin{bmatrix}
	1 & \text{Corr}(x_1, x_2) & \ldots & \text{Corr}(x_1, x_n) \\
	\text{Corr}(x_2, x_1) & 1 & \ldots & \text{Corr}(x_2, x_n) \\
	\vdots & \vdots & \ddots & \vdots \\
	\text{Corr}(x_n, x_1) & \text{Corr}(x_n, x_2) & \ldots & 1
\end{bmatrix}
\end{align}}
\chapter{Variabili campione e propagazione dell'errore statistico}
Ritorniamo però al motivo per cui abbiamo voluto introdurre degli strumenti statistici: ci siamo accorti che l'errore massimo non è adatto per descrivere le incertezze che si compiono effettuando una misura di una determinata grandezza fisica. Adesso però guardiamo la questione da un punto di vista "nuovo": infatti possiamo immaginare che quando misuriamo una grandezza fisica, in particolare quando il valore della misura fluttua, possiamo pensare che il valore stesso sia una variabile aleatoria con una particolare distribuzione (che a priori \textbf{non} è nota) che chiamiamo \emph{distribuzione generatrice}\index{distribuzione generatrice}. \\
In questo schema concettuale fare $n$ misure di una stessa grandezza fisica in condizioni di ripetitività equivale a \emph{campionare} $n$ volte la distribuzione generatrice che caratterizza quel determinato misurando. E' ovvio che non potremo \underline{mai} conoscere completamente la forma della distribuzione generatrice, ma in maniera intuitiva possiamo pensare che, effettuando sempre più misurazioni, inizieremo ad acquisire progressivamente sempre più informazioni su di essa: se pensiamo di fare un numero molto grande di misure (e.g. $n \to +\infty$) e riportiamo i risultati di queste misure in un istogramma allora ci aspettiamo che la forma di questo diventi sempre più simile a quello della distribuzione generatrice.   \\
Possiamo quindi intravedere un nuovo modo per operare, ovvero quello di scrivere come migliore stima della grandezza il valore centrale della distribuzione e come incertezza la deviazione standard. \\
Il nuovo errore si presenterà dunque in questa maniera:
\begin{equation} \label{misura}
	x = \hat{x} \pm \sigma_x \, [ \text{unità di misura} ]
\end{equation}
\section{Campionamenti singoli e ripetuti}
\subsection{Campionamenti singoli}
Se conosciamo a priori la deviazione standard $\sigma$ della distribuzione generatrice del misurando (ma anche avendone una stima) che stiamo, per appunto, \emph{misurando} allora una singola misura (ovvero un singolo campionamento) è sufficiente per definire tutte le componenti della~\ref{misura}: prenderemo il singolo valore misurato come valore centrale e $\sigma$ come incertezza associata. \\
Se conosciamo inoltre la forma della distribuzione possiamo anche determinare il livello di confidenza associato ad una deviazione standard, oppure utilizzare come stima dell'incertezza un multiplo o sottomultiplo della deviazione standard per ottenere un livello di confidenza fissato a priori. \\
\ex{Esempi di campionamenti singoli sapendo la deviazione standard a priori}{\begin{itemize}
	\item Supponiamo di avere un pesi $m$ di un oggetto con una bilancia digitale con una risoluzione di $1 \, \si{\gram}$. Se il valore indicato dal display è $58 \, \si{\gram}$, possiamo assumere che, in assenza di errori sistematici, la distribuzione generatrice del misurando sia uniforme tra $57.5 \, \si{\gram}$ e $58.5 \, \si{\gram}$ e possiamo utilizzare la deviazione standard della funzione uniforme che risulta essere pari a $\sigma = \sqrt{\text{Var}(x)} = \sqrt{\frac{1}{12}} \, \si{\gram}$ e il livello di confidenza (ovvero la probabilità che la variabile disti meno di una deviazione standard) è pari al $58 \%$, dunque
	$$
		m = 58.00 \pm 0.29 \, \si{\gram} \, \, (58 \% \, CL)
	$$
	si osserva infatti che la probabilità che si trovi entro una deviazione standard è pari a $\int\limits_{58-\frac{1}{\sqrt{12}}}^{58+\frac{1}{12}} \frac{1}{(58.5-57.5) \, \si{\gram}} dm \approx 0.577$
	\item Il ragionamento che abbiamo fatto prima si applica alla misura di una lunghezza con il metro a nastro e, più in generale a tutti gli strumenti digitali (se decidiamo di non interpolare tra le divisioni)
	\item In generale gli strumenti si possono \emph{calibrare} tramite misure ripetute di grandezze di riferimento oppure tramite uno strumento di misura più accurato (o usando un metodo indipendente)
\end{itemize}
}
\subsection{Interludio: somma di variabili aleatorie}
\noindent Il problema si pone quando non conosciamo a priori la deviazione standard della nostra misura: come possiamo fare in tal caso?
In primis dobbiamo sapere che cosa accade quando sommiamo due o più variabili casuali: in generale non è banale individuare qual è la "forma" della funzione di distribuzione di $x = \sum\limits_i x_i$, tuttavia possiamo determinare abbastanza facilmente alcuni parametri che caratterizzano la variabile aleatoria $x$, infatti:
$$
	E[x] = E \left[ \sum_i x_i \right] = \sum_i E[x_i] = \sum_i \mu_i
$$
La varianza è più complicata (e ci limiteremo quindi al caso, generalizzabile, di due variabili aleatorie)
\begin{align*}
	&\text{Var}(x) = E[(x-\mu)^2] = E[x^2] - 2\mu E[x] + E[\mu^2] = E[(x_1 + x_2)^2] - 2(\mu_1 + \mu_2)^2 + (\mu_1 + \mu_2)^2 & \\
	&= E[x_1^2] + E[x_2^2] + 2E[x_1 x_2] - (\mu_1 + \mu_2)^2 = \\  &= E[x_1^2] + E[x_2^2] + 2\text{Cov}(x_1, x_2) - \mu_1^2 - \mu_2^2 - 2\mu_1 \mu_2 = E[x_1^2] - \mu_1^2 + E[x_2^2] - \mu_2^2 + 2\text{Cov}(x_1, x_2) = \text{Var}(x_1) + \text{Var}(x_2) + 2\text{Cov}(x_1, x_2)
\end{align*}
Nel caso in cui $x_1$ e $x_2$ siano variabili aleatorie indipendenti, sappiamo che $\text{Cov}(x_1, x_2) = 0$ siccome si ha necessariamente che $E[x_1 x_2] = E[x_1]E[x_2]$, dunque:
\mprop{Valore medio e varianza della somma di variabili indipendenti}{Sia $x = \sum_i x_i$ con $p(x_i)p(x_j)=p(x_i \cap x_j) \forall i \neq j$($x$ è somma di eventi indipendenti) allora
\begin{align*}
	&\mu = \sum_{i = 1} E[x_i] \, &\text{e} \, &\text{Var}(x) = \sqrt{\sum_{i=1} \sigma_i^2}
\end{align*}
}
\begin{myproof}
Si procede per induzione su $n$. Per $n=1$ si osserva che
$$ E[x] = \sum_{i = 1} E[x_i] = \mu_1 $$
adesso mostriamo che $n \implies n+1$:
$$
	E[x] = \sum_{i = 1}^{n+1} E[x_i] = \sum_{i=1}^{n} E[x_i] + \mu_{n+1} = \mu_n + \mu_{n+1} = \sum_{i = 1}^{n+1} \mu_i
$$
Per la varianza si procede alla stessa maniera, sapendo che $\text{Cov}(x_{n}, x_{n+1}) = 0$
\end{myproof}
\noindent Comunque si osservi che
\begin{equation}
	\sqrt{\sum_{i = 1} \sigma_i^2} \leq \sum_{i = 1} \sigma_i
\end{equation}
\begin{myproof}
	Si osservi che siccome $\sigma_i^2 + \sigma_j^2 \leq \sigma_i^2 + \sigma_j^2 + 2\sigma_i \sigma_j = (\sigma_i + \sigma_j)^2$ si deve avere che
	$$
		\sqrt{\sum_{i} \sigma_i^2} \leq \sqrt{ \left( \sum_{i=1} \sigma_i \right)^2} = \sum_{i=1} \sigma_i
	$$
\end{myproof}
\noindent Abbiamo dunque dimostrato che \emph{date $n$ variabili casuali indipendenti la \textbf{media della somma} è uguale alla \textbf{somma delle medie} e la \textbf{varianza della somma} è uguale alla \textbf{somma delle varianze}}
\nt{Un esempio interessante che può essere visto, soprattutto perché utile quando tratteremo la distribuzione di Cauchy, è il cosiddetto \emph{random walk}}
\subsection{Misure ripetute}
Tuttavia torniamo alla questione originale: se abbiamo una serie di misure $x_i \wedge i \in I={1, \ldots, n}$ indipendenti di una stessa grandezza $x$ fatte in condizione di ripetitività, quali sono le migliori stime che possiamo dare della media $\mu$ e della varianza $\sigma^2$ della distribuzione generatrice? Come stima della media possiamo pensare di prendere la media aritmetica delle misure, chiamata \emph{media campione}:
\dfn{Media campione\index{media campione}}{
\begin{equation}
	m = \frac{1}{n} \sum_{i = 1}^{n} x_i
\end{equation}
}
\noindent La media campione ha la proprietà che il suo valore di aspettazione è uguale alla media della distribuzione generatrice:
\begin{equation*}
	E[m] = E \left[ \frac{1}{n} \sum_{i = 1} x_i \right] = \frac{1}{n} \sum_{i = 1} E[x_i] = \frac{1}{n} \cdot n\mu = \mu
\end{equation*}
e un estimatore che soddisfa questa proprietà si dice \emph{imparziale}. \\
La questione della varianza è più complicata siccome l'analogo per la varianza sarebbe la seguente
\begin{equation}
	s^2 = \frac{1}{n} \sum_{i = 1}^n (x_i - \mu)^2
\end{equation}
che si tratta di un estimatore imparziale, siccome si osserva che
$$
	E[s^2] = E \left[ \frac{1}{n} \sum_{i = 1} (x_i - \mu)^2 \right] = \frac{1}{n} E \left[(x_i - \mu)^2 \right] = \sigma^2
$$
ma a priori non conosciamo $\mu$. Come migliore stima della varianza potremmo considerare dunque la \emph{varianza campione}
\dfn{Varianza campione\index{varianza campione}}{
\begin{equation}
	s_n^2 = \frac{1}{n} \sum_{i = 1}^n (x_i - m)^2
\end{equation}
}
E quindi ci potremmo chiedere se il valore di aspettazione della varianza campione sia ancora pari a $\sigma$. Facendo la derivata rispetto ad una generica stima della media, si osserva che
\begin{equation*}
	\frac{d}{d \xi} s_n^2 = -\frac{1}{n} \sum_{i = 1} 2 (x_i - \xi) = \frac{2}{n} \left( \sum_{i=1} x_i - \sum_{i = 1} \xi \right) = \frac{2}{n}(nm - n\xi) = 2(m - \xi)
\end{equation*}
dunque la media campionaria è la media che minimizza la stima della varianza campione, dunque questo ci dice che, in media, $s_n^2$ è una sottostima di $\sigma^2$. Calcoliamo il valore di aspettazione di $s_n^2$, che risulta essere pari a
\begin{equation*}
	E[s_n^2] = E \left[ \frac{1}{n}\sum_{i = 1}^n (x_i - m)^2 \right] = \frac{1}{n} \sum_{i=1}^n E \left[x_i^2 + m^2 -2mx_i \right] = \frac{1}{n} \sum_{i=1}^n \left( E[x_i^2] + E[m^2] - 2E[mx_i] \right)
\end{equation*}
Sapendo che $E[x_i^2] = \sigma^2 + \mu^2$, ci resta da calcolare solo $E[m^2]$ e $E[mx_i]$. Si osserva che $E[m^2]$
\begin{align*}
	&E[m^2] = E \left[ \left( \frac{1}{n}\sum_{i=1}^n x_i \right)^2 \right] = \frac{1}{n^2} E \left[ \left(\sum_{i, j = 1}^n x_ix_j \right) \right] = \frac{1}{n^2} E \left[ \left(\sum_{i = 1}^n x_i^2 + \sum_{i = 1}^n \sum_{j \neq i}^n x_ix_j \right) \right] = \frac{1}{n^2} \left( E \left[ \sum_{i=1}^n x_i^2 \right] + E \left[ \sum_{i=1}^n \sum_{j \neq i}^n x_i x_j \right] \right) = \\
	&= \frac{1}{n^2} \left( n(\sigma^2 + \mu^2) + E \left[ \sum_{i = 1}^n \sum_{j \neq i} x_i x_j \right] \right)
\end{align*}
Si osserva però che la sommatoria $E \left[\sum\limits_{i=1}^n \sum\limits_{j \neq i} x_i x_j \right]$, tramite linearità dell'operatore di aspettazione, può essere trasformata come
$$
E \left[ \sum_{i=1}^n \sum_{j \neq i} x_i x_j \right] = \sum_{i = 1}^n \sum_{j \neq i}^n E[x_i]E[x_j] = \sum_{i = 1}^n (n-1)\mu^2 = n(n-1)\mu^2
$$
dunque, abbiamo che
$$
	E[m^2] = \frac{1}{n^2}[n(\sigma^2 + \mu^2) + n(n-1)\mu^2] = \frac{1}{n^2}[n \sigma^2 + n \mu^2 + n^2 \mu -n \mu^2] = \frac{1}{n}\sigma^2 + \mu^2
$$
Per quando riguarda il valore di aspettazione di $E[mx_i]$ si ha che
$$
E[mx_i] = E \left[ \frac{1}{n} \sum_{j=1}^n x_j x_i \right] = \frac{1}{n} E \left[x_i^2 + \sum_{j\neq i} x_i x_j \right] = \frac{1}{n} \left(E[x_i^2] + \sum_{j \neq i} E[x_i]E[x_j] \right) = \frac{1}{n} \left[\sigma^2 + \mu^2 + (n-1)\mu^2 \right] = \frac{\sigma^2}{n} + \mu^2 
$$
Rimettendo tutto insieme si osserva che nessuno di questi valori dipende dall'indice i, dunque
$$
E[s_n^2] = \frac{1}{n} \sum_{i=1}^n E[x_i^2] + E[m^2] - 2E[mx_i] = \frac{1}{n} \cdot n \left( \frac{\sigma^2}{n} + \mu^2 - 2\frac{\sigma^2}{n} - 2\mu^2 + \sigma^2 + \mu^2 \right) = \frac{n-1}{n}\sigma^2
$$
Tuttavia si osserva che $s_{n}^2$ non è uno stimatore imparziale, ma solo asintoticamente (e.g. se $n \to +\infty$ si ha che $\frac{n-1}{n} \sim 1$) dunque per ovviare a ciò si moltiplica il nostro stimatore $s_n$ per il fattore correttivo $\frac{n}{n-1}$ trasformandolo nello stimatore $s_{n-1}^2$
\begin{equation}
	s_{n-1}^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - m)^2
\end{equation}
\nt{Una domanda che può sorgere è quale delle due stimatori sia più giusto: si tratta di una domanda non banale, tuttavia sappiamo che per un numero molto grande di campioni sono asintoticamente imparziali entrambi, mentre per campioni piccoli la domanda non è scontata. Dai grafici riportati nelle dispense del Baldini si osserva che per $n > 10$ i due stimatori sono compatibili entro il $10 \, \%$ tuttavia per un numero di campioni ancora più basso la domanda è ancora presente. \\
Si deve osservare che il fattore correttivo fa in modo per campioni piccolo lo stimatore $s_n^2$ sia più corretto di quello imparziale, oltre al fatto che rende la coda di destra ancora più pronunciata. Un'altra considerazione che si può fare deriva dal fatto che lo stimatore $s_{n-1}^2$ è uno stimatore imparziale per $\sigma^2$ ma non per $\sigma$
}
\noindent Dunque, dopo essersi dilungati anche fin troppo sugli estimatori statistici, ritorniamo alla domanda iniziale: come posso scrivere il risultato di una misura? \\
Il candidato ideale per il valore centrale sarebbe la media campionaria, mentre per l'incertezza associata non possiamo utilizzare la varianza campione, siccome essa rappresenta le fluttuazione della singola misura (rispetto al valore medio) ma non quella del valore centrale.  \\
Tuttavia la media campione, essendo somma di variabili aleatorie, sarà anch'essa una variabile casuale, dunque se siamo interessati alle sue fluttuazioni
$$
	\text{Var}(m) = \text{Var} \left( \frac{1}{n} \sum_{i = 1}^n x_i \right) = \frac{1}{n^2} \text{Var} \left( \sum_{i=1}^n x_i \right) = \frac{1}{n^2} \cdot n\sigma^2 = \frac{\sigma^2}{n} 
$$
Banalmente, la deviazione standard della media risulta quindi essere
\begin{equation}
	\sigma_m = \sqrt{\frac{\sigma^2}{n}} = \frac{\sigma}{\sqrt{n}}
\end{equation}
e utilizzando la stima della varianza campione possiamo dire che
\begin{equation}
	s_m = \sqrt{\frac{1}{n(n-1)}\sum_{i=1}^n (x_i-m)^2}
\end{equation}
dunque la nostra misura possiamo scriverla come
$$
	x = m \pm s_m
$$
\section{Covarianza e correlazione campione}
Supponiamo di avere una serie di campionamenti $x_i$ ed $y_i$ con $i \in I={1, \ldots, n}$ di due variabili aleatorie $x, y$, ovverosia abbiamo di fatto misurato $n$ coppie ordinate $(x_i, y_i)$. Se indichiamo le medie campionarie, rispettivamente, dalla variabile $x$ e della variabile $y$ con
\begin{align*}
	&m_x = \frac{1}{n} \sum_{i = 1}^n x_i \, &\text{e} \, &m_y  = \frac{1}{n} \sum_{i = 1}^n y_i
\end{align*}
possiamo stimare la covarianza, proprio come abbiamo fatto con la varianza, tramite la covarianza del campione\index{covarianza campione}
\dfn{Covarianza campione}{
\begin{equation}
	q_{xy} = \frac{1}{n-1} \sum_{i=1}^n (x_i - m_x)(y_i - m_y)
\end{equation}
}
\nt{Il termine $n-1$ deriva sempre dalla cosiddetta "\emph{correzione di Bessel}" ovvero quello che abbiamo fatto prima riguardo allo stimatore statistico della varianza campione}
\noindent Dunque la stima delle correlazione $r_{xy}$ campionaria si scrive tramite le stime della varianza campione delle due variabili $x$ e $y$:
\begin{align*}
	&s_x^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - m_x)^2  & &\text{e} & \, &s_y^2 = \frac{1}{n-1} \sum_{i=1}^n (y_i - m_y)^2
\end{align*}
dunque
\begin{equation}
	r_{xy} = \frac{q_{xy}}{s_x s_y} = \frac{\frac{1}{n-1}\sum\limits_{i=1}^n (x_i-m_x)(y_i-m_y)}{\sqrt{\frac{1}{n-1} \sum\limits_{i=1}^n (x_i - m_x)^2 \frac{1}{n-1} \sum\limits_{i=1}^n (y_i - m_y)^2}} = \frac{\sum\limits_{i=1}^n (x_i - m_x)(y_i - m_y)}{\sqrt{\sum\limits_{i=1}^n (x_i - m_x) \sum\limits_{i=1}^n (y_i - m_y)^2}}
\end{equation}
e talvolta viene chiamato anche come \emph{coefficiente di correlazione lineare} o \emph{indice di correlazione di Pearson}: infatti uno dei metodi più semplici per verificare se sussiste una correlazione tra due variabili di un campione è quello di riportare i dati in un grafico di dispersione.
La cosa interessante è il fatto che è possibile, in questa maniera, andare a scrivere la covarianza campione fra due misure come
\begin{equation}
	q_{xy} = r_{xy} s_x s_y
\end{equation}
\section{Media e varianza di una funzione a variabili casuali}
Se abbiamo una generica funzione $f(x)$ come possiamo stimare media e varianza? In generale ci aspettiamo che i valori di $x$ tendano a concentrarsi maggiormente attorno al valore medio, dunque possiamo partire dall'approssimare la funzione tramite sviluppo di Taylor al primo ordine:
$$
	f(x) \approx f(\mu) + \frac{df}{dx}{\Big |}_{x=\mu} (x-\mu)
$$
quindi, si ha che
$$
\mu_f = E[f(x)] \approx E \left[ f(\mu) + \frac{df}{dx}{\Big |}_{x=\mu} (x-\mu) \right] = E[f(\mu)] + E \left[ \frac{df}{dx}{\Big |}_{x=\mu} (x-\mu) \right] = f(\mu) + \frac{df}{dx}{\Big |}_{x = \mu} E[x-\mu] = f(\mu)
$$
dunque abbiamo che
\begin{equation*}
	\mu_f \approx f(\mu)
\end{equation*}
La stima della varianza è leggermente più complicata, ma nel caso ad una singola variabile rimane comunque banale siccome
\begin{align*}
	\sigma_f^2 = E[(f(x) - f(\mu))^2] \approx E \left[ \left( \frac{df}{dx}{\Big |}_{x=\mu}(x-\mu) \right)^2 \right] = \left( \frac{df}{dx} {\Big |}_{x = \mu} \right)^2 E[(x-\mu)^2] = \left( \frac{df}{dx}{\Big |}_{x=\mu} \right)^2 \sigma_x^2
\end{align*}
\dfn{Media e varianza di una generica funzione $f(x)$}{Sia $f(x)$ una generica funzione e $x$ variabile aleatoria, il valore medio di $f(x)$ e la varianza sono rispettivamente
\begin{align*}
	&\mu_f \approx f(\mu) & &\sigma_f^2 \approx \left( \frac{df}{dx} {\Big |}_{x=\mu} \right)^2 \sigma_x^2
\end{align*}
}
\noindent Nel caso di funzioni che dipendono da un certo numero di variabili casuali $x_1$, $x_2, \, \ldots, \, x_n$ con medie $\mu_1, \mu_2, \ldots, \mu_n$ e varianza $\sigma_1^2, \sigma_2^2, \ldots, \sigma_n^2$ la questione diventa più complicata, siccome ritorna in gioco l'indipendenza statistica fa variabili aleatorie. \\
Supponiamo ad esempio di guardare il caso di una funzione $f(x_1, x_2)$ ovvero dipendente da due variabili aleatorie e basta. Innanzitutto, si osserva che attorno al punto medio possiamo approssimare la funzione tramite lo sviluppo di Taylor in due variabili troncato al primo ordine:
$$
f(x_1, x_2) \approx f(\mu_1, \mu_2) + \frac{\partial f}{\partial x_1}{\Big |}_{x_1=\mu_1, x_2=\mu_2}(x_1 - \mu_1) + \frac{\partial f}{\partial x_2}{\Big |}_{x_2=\mu_2, x_2=\mu_2}(x_2 - \mu_2)
$$
dunque possiamo sviluppare il valore medio al primo ordine nella seguente maniera:
\begin{align*}
&\mu_f = E[f(x_1, x_2)] \\
&= E[f(\mu_1, \mu_2)] + \frac{\partial f}{\partial x_1}\bigg|_{\substack{x_1=\mu_1\\x_2=\mu_2}}E[x_1 - \mu_1] + \frac{\partial f}{\partial x_2}\bigg|_{\substack{x_1=\mu_1\\x_2=\mu_2}}E[x_2 - \mu_2] \\
&= f(\mu_1, \mu_2)
\end{align*}
per la varianza la questione è un po' più complicata siccome:
\begin{align*}
&\text{Var}(f(x_1, x_2)) = E[(f(x_1, x_2) - f(\mu_1, \mu_2))^2] = E\left[(\frac{\partial f}{\partial x_1}\bigg|_{\substack{x_1=\mu_1\\x_2=\mu_2}}(x_1 - \mu_1) + \frac{\partial f}{\partial x_2}\bigg|_{\substack{x_1=\mu_1\\x_2=\mu_2}}(x_2 - \mu_2))^2\right] \\
&= \left(\frac{\partial f}{\partial x_1}\bigg|_{\substack{x_1=\mu_1\\x_2=\mu_2}}\right)^2 E[(x_1 - \mu_1)^2] - 2 \frac{\partial f}{\partial x_1}\bigg|_{\substack{x_1=\mu_1\\x_2=\mu_2}} \frac{\partial f}{\partial x_2}\bigg|_{\substack{x_1=\mu_1\\x_2=\mu_2}} E[(x_1 - \mu_1)(x_2 - \mu_2)] + \left(\frac{\partial f}{\partial x_2}\bigg|_{\substack{x_1=\mu_1\\x_2=\mu_2}}\right)^2 E[(x_2 - \mu_2)^2] = \\
&= \left(\frac{\partial f}{\partial x_1}\bigg|_{\substack{x_1=\mu_1\\x_2=\mu_2}}\right)^2 \text{Var}(x_1) + \left(\frac{\partial f}{\partial x_2}\bigg|_{\substack{x_1=\mu_1\\x_2=\mu_2}}\right)^2 \text{Var}(x_2)  - 2 \frac{\partial f}{\partial x_1}\bigg|_{\substack{x_1=\mu_1\\x_2=\mu_2}} \frac{\partial f}{\partial x_2}\bigg|_{\substack{x_1=\mu_1\\x_2=\mu_2}} \text{Cov}(x_1, x_2)
\end{align*}
Naturalmente si semplifica enormemente la formula se $x_1, x_2$ sono variabili fra loro indipendenti, ergo $\text{Cov}(x_1, x_2) = 0$. Si può generalizzare a funzioni dipendenti da $n$ parametri indipendenti che
\begin{equation}
	\sigma_f^2 \approx \sum_{i = 1}^n \left( \frac{\partial f}{\partial x_i} \bigg|_{\substack{x_1=\mu_1 \\ \vdots \\ x_n = \mu_n}})^2 \sigma_i^2 \right)
	\label{eq:prop_stat}
\end{equation}
Questo argomento verrà ripreso più avanti. \\
\section{Propagazione dell'errore statistico}\index{propagazione dell'errore statistico}
Nella teoria dei campioni si è visto che l'incertezza di misura ha il significato di stima della deviazione standard della distribuzione generatrice. Siccome sappiamo calcolare, con le formule che abbiamo visto nella sezione precedente media la deviazione standard di una funzione arbitraria di variabili casuali, date le deviazioni standard delle variabili stesse, siamo adesso in grado di propagare gli errori in maniera statisticamente corretta: supponiamo di avere dunque $n$ grandezze misurare $x_i = \hat{x_i} \pm \sigma_i$ ed una generica funzione $f(x_1, \ldots, x_n)$ e partiamo dal caso più semplice, ovvero quello in cui le grandezze di partenza sono tutte indipendenti. La formula, nella \emph{forma}, è equivalente alla~\ref{eq:prop_stat}, infatti
\dfn{Propagazione dell'errore statistico con variabili indipendenti}{
\begin{equation}
	\sigma_f^2 \approx \sum_{i=1}^n \left( \frac{\partial f}{\partial x_i}\bigg|_{\substack{x_1=\hat{x}_1 \\ \vdots \\ x_n=\hat{x}_n}} \right)^2 \sigma_i^2
\end{equation}
}
sebbene, per quanto riguarda la forma, come ho già detto questa scrittura è equivalente con la~\ref{eq:prop_stat}, dal punto di vista logico non lo sono siccome nella~\ref{eq:prop_stat} le $\sigma_i$ rappresentano in generale le nostre migliori stime delle deviazioni standard delle distribuzioni generatrici e non i valori calcolati a partire dalla forma analitica delle funzioni di distribuzione stesse. \\
Possiamo generalizzare ancora di più: infatti se noi effettuiamo la stima dell'indice di correlazione possiamo dire che
\dfn{Propagazione dell'errore statistico con variabili dipendenti}{
\begin{equation}
	\sigma_f^2 \approx \sum_{i=1}^n \sum_{j = 1}^n \frac{\partial f}{\partial x_i}\bigg|_{\substack{x_1=\hat{x}_1 \\ \vdots \\ x_n=\hat{x}_n}} \frac{\partial f}{\partial x_i}\bigg|_{\substack{x_1=\hat{x}_1 \\ \vdots \\ x_n=\hat{x}_n}} r_{ij}\sigma_i \sigma_j
\end{equation}
}
\chapter{Distribuzioni uni-variate di uso comune}
\section{La distribuzione binomiale}
Supponiamo di considera un esperimento che abbia solamente \textbf{due esiti possibili distinti}, $E_1$ ed $E_2$, con probabilità pari a $p$ e $1-p$ rispettivamente. La domanda che può sorgere spontanea è la seguente: \emph{qual è la probabilità di ottenere $k$ volte l'esito $E_1$ ripetendo l'esperimento $n$ volte e assumendo che le realizzazioni siano indipendenti}? \\
Inizialmente potremmo essere tentati di dire $p^k(1-p)^{n-k}$ ma noi siamo interessati ad una qualunque combinazione degli esiti $E_1$ ed $E_2$, fintanto che avvengano $k$ occorrenze dell'evento $E_1$, pertanto il nostro risultato va moltiplicato per il numero possibile delle combinazioni, pari a $\binom{n}{k}$ (che coincide con il numero di sottoinsiemi di $k$ che è possibile formare in un insieme di $n$ elementi). Possiamo sfruttare il fatto che le probabilità per eventi disgiunti si sommano e possiamo scrivere la probabilità cercata come:
\begin{equation}
	\mathcal{B}(k; n, p) = \binom{n}{k} p^k (1-p)^{n-k}
\end{equation}
\nt{Il punto e virgola nella parentesi degli argomenti di una funzione di distribuzione separa le variabili casuali dai parametri esterni che sono determinati univocamente a priori dal problema}
\subsection{Normalizzazione, media e varianza}
Si dimostra facilmente che la condizione di normalizzazione \index{condizione di normalizzazione} è rispettata.
\thm{Condizione di normalizzazione della distribuzione binomiale}{La distribuzione binomiale rispetta la condizione di normalizzazione, dunque:
\begin{equation}
	\sum_{k=0}^n \mathcal{B}(k;n,p)=1
\end{equation}
}
\begin{myproof}
si osserva banalmente che la sommatoria di $k$ fino ad $n$ rappresenta lo sviluppo della potenza $n$-esima del binomio $p+(1-p)$:
$$
\sum_{k=0}^n \mathcal{B}(k;n,p) = \sum_{k=0}^n \binom{n}{k} p^k (1-p)^{n-k} = (p + 1 - p)^{n} = 1
$$
La tesi è dunque ottenuta
\end{myproof}
\noindent Enunciamo adesso qualche fatto facilmente dimostrabile
\thm{Valore atteso della distribuzione $\mathcal{B}(k; n, p)$}{Il valore atteso della distribuzione binomiale è
	\begin{equation}
		\mu = np
	\end{equation}
}
\begin{myproof} 

\begin{align*}
	\mu = \sum_{k=0}^n k \mathcal{B}(k;n,p) = \sum_{k=0}^n k \cdot \frac{n!}{k!(n-k)!} p^k (1-p)^{n-k} = \sum_{k=0}^n \frac{n(n-1)!}{(k-1)!(n-k)!}p^k(1-p)^{n-k}
\end{align*}
Si osserva che possiamo far partire la sommatoria da $k=1$ siccome il primo termine non contribuisce alla somma ed effettuiamo un banale cambio di indice: definiamo $h=k-1$ e dunque la sommatoria si fermerà a $m=n-1$:
\begin{align*}
	\sum_{k=1}^{n} np \frac{(n-1)!}{(k-1)!(n-k)!}p^k (1-p)^{n-k} = np\sum_{h=0}^{m} \frac{m!}{h!(m-h)!}p^h (1-p)^{m-h}
\end{align*}
Si riconosce nella sommatoria di $\sum\limits_{h=0}^m \frac{m!}{h!(m-h)!}p^h(1-p)^{m-h}$ la condizione di normalizzazione, dunque
$$
	\mu = np\sum_{h=0}^m \frac{m!}{h!(m-h)!} p^h (1-p)^{m-h} = np
$$
\end{myproof}
\thm{Varianza della distribuzione $\mathcal{B}(k;n, p)$}{La varianza della distribuzione binomiale risulta essere
\begin{equation}
	\sigma^2 = np(1-p)
\end{equation}
}
\begin{myproof}
si utilizza la relazione~\ref{calcolo_sigma} per stimare la varianza:
\begin{align*}
&E[k^2] = \sum_{k=0}^n k^2 \mathcal{B}(k;n,p) = \sum_{k=0}^n k^2 \frac{n!}{k!(n-k)!}p^k (1-p)^{n-k} = \sum_{k=1}^n k^2 \frac{n!}{k!(n-k)!}p^k (1-p)^{n-k} \\
&= \sum_{k=1}^n np \frac{(n-1)!}{(k-1)!(n-k)!}p^{k-1}(1-p)^{n-k} = = np\sum_{h=0}^m (h+1)\frac{m!}{h!(m-k)!}p^{h}(1-p)^{m-k} \\
&= np \sum_{h=0}^m h\frac{m!}{h!(m-k)!}p^h (1-p)^{m-k} + np\sum_{h=0}^m \frac{m!}{h!(m-k)!}p^h (1-p)^{m-k} = np(mp + 1) = np(np-p+1)
\end{align*}
dunque, si ha che
$$
\text{Var}(k) = E[k^2]-\mu^2 = np(np-p+1) - n^2p^2 = np(1-p)
$$
La dimostrazione è dunque conclusa.
\end{myproof}
La distribuzione binomiale è asimmetrica generalmente, dunque ha senso chiedere quanto vale la \emph{skewness}. Ricaviamo un'identità molto utile:
\lemma{Momenti di ordine superiore per la $\mathcal{B}(k;n,p)$}{I momento di ordine superiore rispetto la seguente identità:
\begin{equation}
	E[k^{m+1}]=p(1-p)\frac{d}{dp}E[k^m] + npE[k^m]
\end{equation}
}
\begin{myproof}
	\begin{align*}	
	&\frac{d}{dp} E[k^m] = \frac{d}{dp} \sum_{k=0}^n k^m \binom{n}{k}p^k(1-p)^{n-k} = \\
	&=\sum_{k=0}^n k^m \binom{n}{k} \left[ kp^{k-1} (1-p)^{n-k} - p^{k}(n-k)(1-p)^{n-k-1} \right] \\
	&= \sum_{k=0}^n k^{m+1} \binom{n}{k} p^{k-1}(1-p)^{n-k} - \sum_{k=0}^n k^{m} \binom{n}{k} \frac{n-k}{1-p}p^k(1-p)^{n-k} = \\ 
	&= \sum_{k=0}^n k^{m+1} \binom{n}{k} \frac{1}{p} p^{k}(1-p)^{n-k} - \sum_{k=0}^n k^{m} \binom{n}{k} \frac{n}{1-p} p^k(1-p)^{n-k} + \sum_{k=0}^n k^{m+1} \binom{n}{k} p^k(1-p)^{n-k}	= \\
	&= \frac{1}{p} \sum_{k=0}^n k^{m+1} + \sum_{k=0}^n k^{m+1}\binom{n}{k}p^k (1-p)^{n-k} = \\
	&= (\frac{1}{p} + \frac{1}{1-p})E[k^{m+1}] -\frac{n}{1-p}E[k^m]
	\end{align*}
	dunque otteniamo che
	\begin{align*}
	\frac{d}{dp}E[k^m] + \frac{n}{1-p}E[k^m] = \frac{1}{p(1-p)}E[k^{m+1}] \implies E[k^{m+1}] = p(1-p)\frac{d}{dp} E[k^m] + npE[k^m]
	\end{align*}
\end{myproof}
\noindent Tramite questo semplice lemma, possiamo calcolare i momenti superiori al secondo:
\begin{equation}
E[k^3] = p(1-p)\frac{d}{dp}E[k^2] + npE[k^2] = p(1-p)\frac{d}{dp}[np(np - p +1)] + np[p(n-1)+1] = np(1-p)(1-2p)+3n^2p^2(1-p)+n^3p^3
\end{equation}
Dunque, il momento centrale di ordine $3$ risulta essere pari a:
\begin{equation}
	\mu_3 = E[(k-\mu)^3] = E[k^3] - 3\mu\sigma^2 - \mu^3 = np(1-p)(1-2p)+3n^2p^2(1-p) + n^3p^3-3n^2p^2(1-p)-n^3p^3 = np(1-p)(1-2p)
\end{equation}
dunque si ha che la \emph{skewness} $\gamma_1$:
$$
\gamma_1 = \frac{\mu_3}{\sigma^3} = \frac{1-2p}{\sqrt{np(1-p)}}
$$
\section{La distribuzione di Poisson}
Formalmente la distribuzione di Poisson può essere ottenuta come limite della binomiale per $p \to 0$ e per $np =  \mu$ (dunque si ha che $p=\frac{\mu}{n}$ dunque il numero di ripetizioni $n$ dev'essere molto grande.
\chapter{Funzioni di probabilità di funzioni qualunque} 
\section{}
Nel momento in cui si considera $f(x)$ dove $x$ è una variabile aleatoria, la funzione di distribuzione che caratterizza la funzione $f(x)$, nel caso in cui $f$ risulti biunivoca, è, banalmente, pari a
$$
	p(y)=p(x)
$$
tuttavia nel caso di funzioni iniettive, la questione si complica.
\pagebreak
\printindex
\end{document}
